[
  {
    "objectID": "pages/blog.html",
    "href": "pages/blog.html",
    "title": "Probably a blog about modelling, probabilistic programming, scientific computing, and research workflows.",
    "section": "",
    "text": "Setup\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 1, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping 101\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ndata collection\n\n\nweb scraping\n\n\napis\n\n\ntutorial\n\n\n\nModule 2.1, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with the YouTube API\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ndata collection\n\n\nweb scraping\n\n\napis\n\n\ntutorial\n\n\n\nModule 2.2, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nText Analysis Foundations\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 3, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nText Analysis with Transformers\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 3, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Analysis with Graph-Tool (Political Blogs Network)\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 4, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Analysis with Graph-Tool (Enron Email Network)\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 4, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation and Agent-based Modelling\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 5, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nProject Work\n\n\n\n\n\n\nPython\n\n\nGESIS\n\n\ncomputational social science\n\n\ndata science\n\n\ntutorial\n\n\n\nModule 6, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024\n\n\n\n\n\nAug 26, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nHello World!\n\n\n\n\n\nThese are the first words from every blog, right?\n\n\n\n\n\nFeb 19, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\n\n\n\n\n\n\nHow I built this site\n\n\n\n\n\nJust testing… sorry, come back later.\n\n\n\n\n\nFeb 19, 2024\n\n\nJohn McLevey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "Dr. John McLevey (he/him)",
    "section": "",
    "text": "My current research addresses three broad and longstanding themes that cut across the social, cognitive, and computational/statistical sciences. First, I am developing generative models of schematic cognition, social influence, and cultural learning to deepen our knowledge of how latent opinions, identities, beliefs, and behaviors form and co-evolve, and to better measure polarization and large-scale cultural change. In developing these models, I am primarily focused on culture related to (a) climate change and environmental policy; (a) harm reduction initiatives and public health responses to the toxic drug supply and overdose crisis; (c) generative artificial intelligence; (d) privacy, security, and surveillance; and (e) lifestyle preferences and politics.\nSecond, I am using these models as digital ’laboratories’ (of a sort) to better understand the workings and impacts of coordinated information operations such as disinformation campaigns and censorship on populations. Third, I am using data from social media platforms to better understand the role of emotional dynamics and identity-related processes in online political discussions. I approach each of these projects as a computational social scientist with expertise in network science and social network analysis, probabilistic and generative modelling, and computational text analysis.\nMy work is funded by research grants from the Social Science and Humanities Research Council of Canada (SSHRC) and an Early Researcher Award from the Ontario Ministry of Research and Innovation. To learn more, browse my CV (with links to preprints and open access publications) or check out my books.\nOutside of work and spending time with friends and family, I generally spend my time playing basketball, hiking, strength training, traveling, and reading. When I am not traveling for work, I split my time between Waterloo (ON), Toronto (ON), and St. John’s (NL). However, nearly all of this is temporarily on hold as my partner and I adjust to life with newborn identical twins!"
  },
  {
    "objectID": "pages/about.html#education",
    "href": "pages/about.html#education",
    "title": "Dr. John McLevey (he/him)",
    "section": "Education",
    "text": "Education\n\nPhD, Sociology, McMaster University (2009-2013)\nMA, Sociology, McMaster University (2008-2009)\nBA(H) Sociology, Political Science, Memorial University (2004-2008)"
  },
  {
    "objectID": "pages/about.html#expertise",
    "href": "pages/about.html#expertise",
    "title": "Dr. John McLevey (he/him)",
    "section": "Expertise",
    "text": "Expertise\nComputational Social Science, Generative Models, Natural Language Processing (NLP), Network Science, Cognitive Science, Cultural Cognition and Opinion Dynamics, Environmental Sociology, Sociology of Science and Technology"
  },
  {
    "objectID": "pages/consulting.html",
    "href": "pages/consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "Consulting"
  },
  {
    "objectID": "pages/books.html",
    "href": "pages/books.html",
    "title": "Books",
    "section": "",
    "text": "The SAGE Handbook of Social Network Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis new edition of The Sage Handbook of Social Network Analysis builds on the success of its predecessor, offering a comprehensive overview of social network analysis produced by leading international scholars in the field. Brand new chapters provide both significant updates to topics covered in the first edition, as well as discussing cutting edge topics that have developed since, including new chapters on (1) general issues such as social categories and computational social science; (2) applications in contexts such as environmental policy, gender, ethnicity, cognition and social media and digital networks; and (3) concepts and methods such as centrality, blockmodeling, multilevel network analysis, spatial analysis, data collection, and beyond.  Learn more about the book here!\n\n\n\n\n\nDoing Computational Social Science: A Practical Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputational approaches offer exciting opportunities for us to do social science differently. This beginner’s guide discusses a range of computational methods and how to use them to study the problems and questions you want to research. It assumes no knowledge of programming, offering step-by-step guidance for coding in Python and drawing on examples of real data analysis to demonstrate how you can apply each approach in any discipline. […] For anyone who wants to use computational methods to conduct a social science research project, this book equips you with the skills, good habits and best working practices to do rigorous, high quality work.  Learn more about the book here!\n\n\n\n\n\nFace-to-Face: Science, Trust, Democracy, and the Internet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe internet is changing the way that knowledge is made and shared. Knowledge-making in face-to-face settings is being replaced by information gathering from remote sources, whose origins may be concealed but which can create an illusion of intimacy. Though remote communication is beneficial in many ways – modern societies would fail without it – too much reliance on remote communication threatens the core institutions of democratic societies. […] By drawing out the special features of face-to-face interaction and its constitutive role in creating societies, with science as the icon, the book sets out an agenda for civic education that can protect democratic institutions from the erosion of pluralism and the facile abandonment of trustworthy expertise.  Learn more about the book here!\n\n\n\n\n\nIndustrial Development and Eco Tourisms: Can Oil Extraction and Nature Convervation Coexist?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis book examines the “oil-tourism interface”, the broad range of direct and indirect contact points between offshore oil extraction and nature-based tourism. Offshore oil extraction and nature-based tourism are pursued as development paths across the North Atlantic region. Offshore oil promises economic benefits from employment and royalty payments to host societies, but is based on fossil fuel-intensive resource extraction. Nature-based tourism, instead, is based on experiencing natural environments and encountering wildlife, including whales, seals, or seabirds. […] Through a comparative analysis of Denmark, Iceland, Newfoundland and Labrador, Norway, and Scotland, this book offers important lessons for how coastal societies can better navigate relationships between resource extraction and nature-based tourism in the interests of social-ecological wellbeing.  Learn more about the book here!"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Dr. John McLevey (he/him)",
    "section": "About",
    "text": "About\nMy current research addresses three broad and longstanding themes that cut across the social, cognitive, and computational/statistical sciences. First, I am developing generative models of schematic cognition, social influence, and cultural learning to deepen our knowledge of how latent opinions, identities, beliefs, and behaviors form and co-evolve, and to better measure polarization and large-scale cultural change. In developing these models, I am primarily focused on culture related to (a) climate change and environmental policy; (a) harm reduction initiatives and public health responses to the toxic drug supply and overdose crisis; (c) generative artificial intelligence; (d) privacy, security, and surveillance; and (e) lifestyle preferences and politics.\nSecond, I am using these models as digital ’laboratories’ (of a sort) to better understand the workings and impacts of coordinated information operations such as disinformation campaigns and censorship on populations. Third, I am using data from social media platforms to better understand the role of emotional dynamics and identity-related processes in online political discussions. I approach each of these projects as a computational social scientist with expertise in network science and social network analysis, probabilistic and generative modelling, and computational text analysis.\nMy work is funded by research grants from the Social Science and Humanities Research Council of Canada (SSHRC) and an Early Researcher Award from the Ontario Ministry of Research and Innovation. To learn more, browse my CV (with links to preprints and open access publications) or check out my books.\nOutside of work and spending time with friends and family, I generally spend my time playing basketball, hiking, strength training, traveling, and reading. When I am not traveling for work, I split my time between Waterloo (ON), Toronto (ON), and St. John’s (NL). However, nearly all of this is temporarily on hold as my partner and I adjust to life with newborn identical twins!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dr. John McLevey (he/him)",
    "section": "Education",
    "text": "Education\n\nPhD, Sociology, McMaster University (2009-2013)\nMA, Sociology, McMaster University (2008-2009)\nBA(H) Sociology, Political Science, Memorial University (2004-2008)"
  },
  {
    "objectID": "index.html#expertise",
    "href": "index.html#expertise",
    "title": "Dr. John McLevey (he/him)",
    "section": "Expertise",
    "text": "Expertise\nComputational Social Science; Network Science; Generative Modelling; Natural Language Processing; Cognitive Science; Cultural Cognition, Emotion, and Opinion Dynamics; Sociology of Science and Technology; Environmental Sociology"
  },
  {
    "objectID": "publications/books/Face-to-Face.html#part-i-foundations-communication-socialization-and-trust",
    "href": "publications/books/Face-to-Face.html#part-i-foundations-communication-socialization-and-trust",
    "title": "Face-to-Face: Science, Trust, Democracy, and the Internet",
    "section": "Part I: Foundations: Communication, Socialization and Trust",
    "text": "Part I: Foundations: Communication, Socialization and Trust\nChapter 1 | What Trust and Communication Are For\nChapter 2 | Forming Societies and Learning to Trust and to Rely\nChapter 3 | Completing the Story of Face-to-Face Communication"
  },
  {
    "objectID": "publications/books/Face-to-Face.html#part-ii-arguments-and-evidence-can-remote-communication-replace-face-to-face",
    "href": "publications/books/Face-to-Face.html#part-ii-arguments-and-evidence-can-remote-communication-replace-face-to-face",
    "title": "Face-to-Face: Science, Trust, Democracy, and the Internet",
    "section": "Part II: Arguments and Evidence: Can Remote Communication Replace Face to Face?",
    "text": "Part II: Arguments and Evidence: Can Remote Communication Replace Face to Face?\nChapter 4 | Remote Technology and Trust\nChapter 5 | Can Remote Replace Face-to-Face Communication?\nChapter 6 | Small Groups to Big Groups: When Big Groups Are Trustworthy\nChapter 7 | The ‘Stickiness’ of Face-to-Face Communication: Some Case Studies\nChapter 8 | When Remote Communication Is Not Trustworthy\nChapter 9 | Disinformation and Misinformation"
  },
  {
    "objectID": "publications/books/Face-to-Face.html#part-iii-consequences-science-truth-democracy-and-the-nature-of-society",
    "href": "publications/books/Face-to-Face.html#part-iii-consequences-science-truth-democracy-and-the-nature-of-society",
    "title": "Face-to-Face: Science, Trust, Democracy, and the Internet",
    "section": "Part III: Consequences: Science, Truth, Democracy and the Nature of Society",
    "text": "Part III: Consequences: Science, Truth, Democracy and the Nature of Society\nChapter 10 | Some Immediate Consequences of the Coronavirus (COVID-19) Pandemic for Science\nChapter 11 | The Nature of Democracy and Scientific Expertise\nChapter 12 | What Is to Be Done?\nPostscript | The November 2020 Election in the USA"
  },
  {
    "objectID": "publications/books/Face-to-Face.html#appendixes",
    "href": "publications/books/Face-to-Face.html#appendixes",
    "title": "Face-to-Face: Science, Trust, Democracy, and the Internet",
    "section": "Appendixes",
    "text": "Appendixes\nAppendix A | Propaganda and Other Traditions\nAppendix B | (i) Coronavirus (COVID-19) Disinformation, (ii) Update on Disinformation in General, and (iii) a Warning about How Not to Fix the Problem\nAppendix C | The Delineated Cases of Citizen Participation in Science and Technology\nAppendix D | An Alternative View: Successful Business Interaction Without Face-to-Face Communication\nAppendix E | Second Language Learning"
  },
  {
    "objectID": "publications/books/Industrial-Development-Eco-Tourisms.html",
    "href": "publications/books/Industrial-Development-Eco-Tourisms.html",
    "title": "Industrial Development and Eco Tourisms: Can Oil Extraction and Nature Convervation Coexist?",
    "section": "",
    "text": "Description\nThis book examines the “oil-tourism interface”, the broad range of direct and indirect contact points between offshore oil extraction and nature-based tourism. Offshore oil extraction and nature-based tourism are pursued as development paths across the North Atlantic region. Offshore oil promises economic benefits from employment and royalty payments to host societies, but is based on fossil fuel-intensive resource extraction. Nature-based tourism, instead, is based on experiencing natural environments and encountering wildlife, including whales, seals, or seabirds. They share social-ecological space, such as oceans, coastlines, cities and towns where tourism and offshore oil operations and offices are located. However, they rarely share cultural or political space, in terms of media coverage, public debate, or policy discussion that integrates both modes of development. Through a comparative analysis of Denmark, Iceland, Newfoundland and Labrador, Norway, and Scotland, this book offers important lessons for how coastal societies can better navigate relationships between resource extraction and nature-based tourism in the interests of social-ecological wellbeing.\n\n\n\n \n\n\n\n\nContents\nChapter 1 | Introduction: Contact Points Between Offshore Oil and Nature-Based Tourism\nChapter 2 | The North Atlantic as Object of Inquiry\nChapter 3 | Cultural Dimensions of the Oil-Tourism Interface\nChapter 4 | Environmental Governance and the Oil-Tourism Interface\nChapter 5 | Environmental Movement Conflict and Collaboration in the Oil-Tourism Interface\nChapter 6 | Lessons Learned and Social Futures: Building Social-Ecological Wellbeing in Coastal Communities\nChapter 7 | Epilogue on Methodology\n\n\nReviews\n\n“This book breaks new ground with its introduction of the notion of the oil-tourism interface. Through this analytical and empirical lens, the book explores the tensions between oil extraction and eco-tourism – in terms of extractive and attractive development. The devastating impact of oil extraction on the environment also has detrimental consequences for highly-valued tourism landscapes. One of the main strengths of this book lies in the rich cases, which the authors use to illustrate the complexities of these consequences and the ways in which societies, including policy-makers, business, citizens and social movements, evaluate and try to shape tourism and oil development models. These insights are important in the current context of climate crisis, which both impacts and is impacted by oil extraction and eco-tourism.” Julie Uldam, Associate Professor, Department of Management, Society and Communication Copenhagen Business School, Denmark\n\n\n“The ambivalent relations between fossil fuel extraction and tourism are a particularly apposite research topic in the North Atlantic, where global climate change can be seen at work in often poignant ways. The authors develop a relational political ecology approach which, combined with an ‘omnivorous methodology’, lends itself well to a nuanced analysis of the oil-tourism interface. This book is a valuable addition to environmental social science scholarship and essential reading for those interested in the complexities of a rapidly changing region.” Karl Benediktsson, Professor of Human Geography, University of Iceland, Iceland\n\n\n“Around the world, the same landscapes that harbor mineral riches are often tourist attractions, setting up tough policy dilemmas. The authors wisely build a framework for understanding and assessing these policies through the concept of”social-ecological wellbeing,” applied to fascinating cases on the coasts on both sides of the North Atlantic. Anyone interested in the future of the planet will learn a lot from this book of political ecology.” James M. Jasper, Author of The Art of Moral Protest\n\n\n“Eco-tourism and extractive industries may seem unlikely bedfellows but the reality is that, in many parts of the world, governments pursue both. By examining the politics and cultures of oil and tourism in the North Atlantic, Stoddart, Mattoni and McLevy shed new light on how contradictions and conflicts between the two development paths are managed and, critically, where opportunities to promote more positive social and ecological futures lie. I thoroughly recommend it.” Stewart Lockie, Distinguished Professor of Sociology, James Cook University, Australia\n\n\n“In this sweeping five-nation comparison, the authors untangle the complicated interplay of offshore oil extraction and nature-based tourism for the coastal cultures and political interests of the North Atlantic region. Through the adoption of an “omnivorous” methodological approach, the authors identify how these two industries have co-existed in isolation, cooperation, and conflict. From their findings, they offer a compelling argument for the consistent inclusion of environmental groups in deliberations on these economic activities at sea, ideally for the enhanced wellbeing of North Atlantic communities and the marine environment. Their assessment is particularly salient given climate change and the future significance of the Arctic in both oil exploration and tourism.” Patricia Widener, Florida Atlantic University, USA, author of Toxic and Intoxicating Oil\n\n\n\n\n\nCitationBibTeX citation:@book{cj_stoddart2020,\n  author = {CJ Stoddart, Mark and Mattoni, Alice and McLevey, John},\n  publisher = {Palgrave Macmillan},\n  title = {Industrial {Development} and {Eco} {Tourisms:} {Can} {Oil}\n    {Extraction} and {Nature} {Convervation} {Coexist?}},\n  date = {2020-11-01},\n  address = {London, UK},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCJ Stoddart, Mark, Alice Mattoni, and John McLevey. 2020. Industrial\nDevelopment and Eco Tourisms: Can Oil Extraction and Nature Convervation\nCoexist? London, UK: Palgrave Macmillan."
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html",
    "title": "Web Scraping 101",
    "section": "",
    "text": "In this tutorial, you’ll learn some foundational web scraping skills using Python, focusing specifically on working with static web pages. To help you develop your skills, we’ll work through an extended example of extracting data from the World Happiness Report Wikipedia page. We’ll start with the basics – loading and parsing HTML – and gradually move on to more complex tasks like handling nested tables and creating visualizations from the data we collect. By the end of this tutorial, you know how to navigate the structure of an HTML page, extract specific pieces of data such as headers, body text, and tables, and clean the data for analysis.\n\n\nIn this tutorial, you will learn how to:\n\nLoad a static website using requests to obtain the HTML content of a webpage.\nParse HTML using BeautifulSoup to structure and navigate the content.\nExtract headers and body text from a webpage for further analysis.\nExtract and process links from the webpage, including differentiating between relative, full, and internal links.\nCreate a DataFrame from extracted data, making it easier to analyze and visualize.\nHandle and clean HTML tables, including dealing with nested tables that may complicate data extraction.\nVisualize data by creating simple plots from the cleaned and processed information."
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#learning-objectives",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#learning-objectives",
    "title": "Web Scraping 101",
    "section": "",
    "text": "In this tutorial, you will learn how to:\n\nLoad a static website using requests to obtain the HTML content of a webpage.\nParse HTML using BeautifulSoup to structure and navigate the content.\nExtract headers and body text from a webpage for further analysis.\nExtract and process links from the webpage, including differentiating between relative, full, and internal links.\nCreate a DataFrame from extracted data, making it easier to analyze and visualize.\nHandle and clean HTML tables, including dealing with nested tables that may complicate data extraction.\nVisualize data by creating simple plots from the cleaned and processed information."
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#setup",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#setup",
    "title": "Web Scraping 101",
    "section": "Setup",
    "text": "Setup\nAs always, we’ll start by importing packages, including\n\nrequests for making HTTP requests,\nurllib for processing URL data, and\nBeautifulSoup from the bs4 package for parsing HTML.\n\n\nfrom collections import Counter\nimport re\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\n\nfrom icsspy import set_style\n\nset_style()"
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#loading-static-websites-with-requests",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#loading-static-websites-with-requests",
    "title": "Web Scraping 101",
    "section": "Loading Static Websites with requests",
    "text": "Loading Static Websites with requests\nThe first step in scraping a website is to inspect its source code in your browser and load its content in a way Python can access. We can do the latter using the requests library. The get() function sends a request to the specified URL and returns a response object, which contains all the information sent back from the server, including the HTML source code.\n\nurl = \"https://en.wikipedia.org/w/index.php?title=World_Happiness_Report&oldid=1241093905\"\n\nresponse = requests.get(url)\nresponse\n\n&lt;Response [200]&gt;\n\n\nIf you print response.text, you’ll see the raw HTML of the webpage. This is the data we’ll be working with in the rest of this tutorial."
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#parsing-html-with-beautifulsoup",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#parsing-html-with-beautifulsoup",
    "title": "Web Scraping 101",
    "section": "Parsing HTML with BeautifulSoup",
    "text": "Parsing HTML with BeautifulSoup\nNow that we have the HTML content, we need to parse it into a structured format. BeautifulSoup helps us do this by converting the HTML string into a navigable tree structure. This allows us to easily search for and extract specific elements, such as headers, paragraphs, and links.\n\nExtracting Text with BeautifulSoup\nFirst, we create a BeautifulSoup object by passing the HTML content to the BeautifulSoup constructor along with the desired parser. We’ll use the html.parser, which is the default option.\n\nhtml_content = response.text\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n\n\nExtracting Headers\nTo extract headers, we can search the soup object for all header tags (h1 to h6). Below, we loop through the results and extract the text content of each header.1\n1 By default, the .get_text() methods removes leading and training whitespace from each header. If we want to be more aggressive about removing whitespace (e.g., replace double spaces with single spaces), we can set the .get_text() method’s strip argument to True.\nheaders = []\n\nfor header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n    headers.append(header.get_text())\n\nheaders\n\n['Contents',\n 'World Happiness Report',\n 'History',\n 'Methods and philosophy',\n 'WELLBYs',\n 'Happiness of the young and old',\n 'Annual report topics',\n '2024 World Happiness Report',\n '2023 World Happiness Report',\n '2022 World Happiness Report',\n '2021 World Happiness Report',\n '2020 World Happiness Report',\n '2019 World Happiness Report',\n '2018 World Happiness Report',\n '2017 World Happiness Report',\n '2016 World Happiness Report',\n '2015 World Happiness Report',\n '2013 World Happiness Report',\n '2012 World Happiness Report',\n 'International rankings',\n '2024 report',\n '2023 report',\n '2022 report',\n '2020 report',\n '2019 report',\n '2018 report',\n '2017 report',\n '2016 report',\n '2013 report',\n 'Criticism',\n 'Metrics',\n 'Methodology',\n 'Legitimacy',\n 'See also',\n 'Notes',\n 'References',\n 'External links']\n\n\nIn my experience, this argument often ends up removing meaningful whitespace, which can causes headaches downstream when doing things like text analysis. For that reason, I prefer not to use it and simply do additional cleaning if/when it is needed.\nThis code collects all headers into a list.\n\n\nExtracting Body Text\nSimilarly, we can extract all paragraphs (&lt;p&gt; tags) from the HTML document. We’ll collect the text from each paragraph into a list.\n\nbody_text = []\nfor paragraph in soup.find_all(['p']):\n    body_text.append(paragraph.get_text())\n\nprint(f'Found {len(body_text)} paragraphs')\n\nFound 88 paragraphs\n\n\nThere are 88 paragraphs in this document. Let’s preview the first 5.\n\nbody_text[:5]\n\n['This is an old revision of this page, as edited by 41.189.206.7 (talk) at 08:42, 19 August 2024 (→\\u200e2023 report). The present address (URL) is a permanent link to this revision, which may differ significantly from the current revision.',\n 'The World Happiness Report is a publication that contains articles and rankings of national happiness, based on respondent ratings of their own lives,[1] which the report also correlates with various (quality of) life factors.[2] The report primarily uses data from the Gallup World Poll. As of March 2024, Finland has been ranked the happiest country in the world seven times in a row.[3][4][5][6][7]\\n',\n 'Since 2024, the report has been published under a partnership between Gallup, the Wellbeing Research Centre at the University of Oxford, and the UN Sustainable Development Solutions Network.[8] The editorial team includes three founding editors, John F. Helliwell, Richard Layard, and Jeffrey D. Sachs, and editors, Jan-Emmanuel De Neve, Lara Aknin, and Shun Wang.[9]\\n',\n 'In July 2011, the UN General Assembly adopted resolution 65/309 Happiness: Towards a Holistic Definition of Development[10] inviting member countries to measure the happiness of their people and to use the data to help guide public policy. \\n',\n 'The first World Happiness Report was released on 1 April 2012, as a foundational text for the UN High Level Meeting: Well-being and Happiness: Defining a New Economic Paradigm,[11] drawing international attention.[12] On 2 April 2012, this was followed by the first UN High Level Meeting called Wellbeing and Happiness: Defining a New Economic Paradigm,[13] which was chaired by UN Secretary General Ban Ki-moon and Prime Minister Jigmi Thinley of Bhutan, a nation that adopted gross national happiness instead of gross domestic product as their main development indicator.[14]\\n']\n\n\nExtracting text like this is useful for text analysis, which we’ll cover in the next module. For now, we’ll create a dataframe from this data\n\nordered_text = pd.DataFrame(body_text)\nordered_text.columns = ['paragraph']\nordered_text['sequence'] = ordered_text.index.to_list()\n\nordered_text.head(10)\n\n\n\n\n\n\n\n\nparagraph\nsequence\n\n\n\n\n0\nThis is an old revision of this page, as edite...\n0\n\n\n1\nThe World Happiness Report is a publication th...\n1\n\n\n2\nSince 2024, the report has been published unde...\n2\n\n\n3\nIn July 2011, the UN General Assembly adopted ...\n3\n\n\n4\nThe first World Happiness Report was released ...\n4\n\n\n5\nThe first report outlined the state of world h...\n5\n\n\n6\nThe rankings of national happiness are based o...\n6\n\n\n7\nThe life factor variables used in the reports ...\n7\n\n\n8\nThe use of subjective measurements of wellbein...\n8\n\n\n9\nIn the reports, experts in fields including ec...\n9\n\n\n\n\n\n\n\nand write it to disk for later use.\n\nordered_text.to_csv(\n  'output/happiness_report_wikipedia_paragraphs.csv', index=False\n)\n\n\n\nExtracting Links\nNext, let’s extract all the links on the page. Once again, we can use the find_all() method to find all the anchor tags (&lt;a&gt;), i.e., links.\n\nurls = []\nfor link in soup.find_all('a'):\n    url = link.get('href')\n    \n    # check if valid href\n    if url: \n        urls.append(url)\n        \nprint(f'Found {len(urls)} urls')\n\nFound 2052 urls\n\n\nWhen we print these URLs, we’ll see that many are relative links (i.e., links to other Wikipedia pages that start with /). These links won’t work properly if clicked as-is because they’re not full URLs.\n\nfor url in urls[:10]:\n    print(url)\n\n#bodyContent\n/wiki/Main_Page\n/wiki/Wikipedia:Contents\n/wiki/Portal:Current_events\n/wiki/Special:Random\n/wiki/Wikipedia:About\n//en.wikipedia.org/wiki/Wikipedia:Contact_us\nhttps://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n/wiki/Help:Contents\n/wiki/Help:Introduction\n\n\nTo fix this, we need to prepend the base URL for Wikipedia (https://en.wikipedia.org) to any relative URLs. Let’s iterate over the URLs, identify which ones are relative, and prepend the base URL to them. We’ll create a new list of URLs called full_urls and append each all our full URLs, whether it was full from the start or expanded by prepending the base URL.\n\nbase_url = \"https://en.wikipedia.org\"\n\nfull_urls = []\nfor url in urls:\n    if url.startswith('/'): # relative URL\n        full_url = urljoin(base_url, url)\n    else:\n        full_url = url\n    full_urls.append(full_url)\n\nLet’s see what they look like now.\n\nfor url in full_urls[:10]:\n    print(url)\n\n#bodyContent\nhttps://en.wikipedia.org/wiki/Main_Page\nhttps://en.wikipedia.org/wiki/Wikipedia:Contents\nhttps://en.wikipedia.org/wiki/Portal:Current_events\nhttps://en.wikipedia.org/wiki/Special:Random\nhttps://en.wikipedia.org/wiki/Wikipedia:About\nhttps://en.wikipedia.org/wiki/Wikipedia:Contact_us\nhttps://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\nhttps://en.wikipedia.org/wiki/Help:Contents\nhttps://en.wikipedia.org/wiki/Help:Introduction\n\n\n\nProcessing Links\nNext, we’ll filter out any internal page links (e.g., links to sections within the same page) by excluding URLs that start with #.\n\nprint(f'{len(full_urls)} URLs before removing internal page links.')\nfull_urls = [url for url in full_urls if not url.startswith('#')]\nprint(f'{len(full_urls)} URLs after removing internal page links.')\n\n2052 URLs before removing internal page links.\n1857 URLs after removing internal page links.\n\n\n\nfor url in full_urls[:10]:\n    print(url)\n\nhttps://en.wikipedia.org/wiki/Main_Page\nhttps://en.wikipedia.org/wiki/Wikipedia:Contents\nhttps://en.wikipedia.org/wiki/Portal:Current_events\nhttps://en.wikipedia.org/wiki/Special:Random\nhttps://en.wikipedia.org/wiki/Wikipedia:About\nhttps://en.wikipedia.org/wiki/Wikipedia:Contact_us\nhttps://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\nhttps://en.wikipedia.org/wiki/Help:Contents\nhttps://en.wikipedia.org/wiki/Help:Introduction\nhttps://en.wikipedia.org/wiki/Wikipedia:Community_portal\n\n\n\n\nCreating a DataFrame from Links\nNow, let’s use what we’ve learned to write a clean code block that:\n\nFinds all the a tags (URLs)\nIgnores internal links to sections of the source document\nAdds the base Wikipedia URL to any relative links\nCollects the link data into a list called link_data\nCreates a dataframe from link_data\n\nAdditionally, we’ll include a Boolean column indicating whether the link is external (True for external, False otherwise).\n\nlink_data = []\n\nfor link in soup.find_all('a', href=True):\n    if not link['href'].startswith('#'): # ignore internal links\n        text = link.get_text()\n        href = link['href']\n        if href.startswith('/'): # add base URL to relative links\n            href = \"https://en.wikipedia.org\" + href\n            link_data.append((text, href, False))\n        else:\n            link_data.append((text, href, True))\n\nLet’s create a dataframe from this list.\n\nlink_df = pd.DataFrame(link_data, columns=['link-text', 'href', \"external\"])\nlink_df.head(10)\n\n\n\n\n\n\n\n\nlink-text\nhref\nexternal\n\n\n\n\n0\nMain page\nhttps://en.wikipedia.org/wiki/Main_Page\nFalse\n\n\n1\nContents\nhttps://en.wikipedia.org/wiki/Wikipedia:Contents\nFalse\n\n\n2\nCurrent events\nhttps://en.wikipedia.org/wiki/Portal:Current_e...\nFalse\n\n\n3\nRandom article\nhttps://en.wikipedia.org/wiki/Special:Random\nFalse\n\n\n4\nAbout Wikipedia\nhttps://en.wikipedia.org/wiki/Wikipedia:About\nFalse\n\n\n5\nContact us\nhttps://en.wikipedia.org//en.wikipedia.org/wik...\nFalse\n\n\n6\nDonate\nhttps://donate.wikimedia.org/wiki/Special:Fund...\nTrue\n\n\n7\nHelp\nhttps://en.wikipedia.org/wiki/Help:Contents\nFalse\n\n\n8\nLearn to edit\nhttps://en.wikipedia.org/wiki/Help:Introduction\nFalse\n\n\n9\nCommunity portal\nhttps://en.wikipedia.org/wiki/Wikipedia:Commun...\nFalse\n\n\n\n\n\n\n\nNow, let’s count the number of internal and external links.\n\nlink_df['external'].value_counts().reset_index()\n\n\n\n\n\n\n\n\nexternal\ncount\n\n\n\n\n0\nFalse\n1713\n\n\n1\nTrue\n144\n\n\n\n\n\n\n\n\n\nExtracting Primary Domains\nLet’s extract and count primary domains for external links. We’ll define a simple function to do this.\n\ndef extract_primary_domain(url):\n    \"\"\"\n    Extracts the primary domain from a URL by splitting the netloc by '.' \n    and taking the second-to-last element. Returns the primary domain.\n    \"\"\"\n    netloc = urlparse(url).netloc\n    parts = netloc.split('.')\n    if len(parts) &gt; 1:\n        primary_domain = parts[-2]\n    else:\n        primary_domain = parts[0]  \n    return primary_domain\n\nWe can use the apply() methods for dataframes to apply our function to each row in the href column and then add the extracted primary domain to a new column.\n\nlink_df['primary_domain'] = link_df['href'].apply(extract_primary_domain)\nlink_df\n\n\n\n\n\n\n\n\nlink-text\nhref\nexternal\nprimary_domain\n\n\n\n\n0\nMain page\nhttps://en.wikipedia.org/wiki/Main_Page\nFalse\nwikipedia\n\n\n1\nContents\nhttps://en.wikipedia.org/wiki/Wikipedia:Contents\nFalse\nwikipedia\n\n\n2\nCurrent events\nhttps://en.wikipedia.org/wiki/Portal:Current_e...\nFalse\nwikipedia\n\n\n3\nRandom article\nhttps://en.wikipedia.org/wiki/Special:Random\nFalse\nwikipedia\n\n\n4\nAbout Wikipedia\nhttps://en.wikipedia.org/wiki/Wikipedia:About\nFalse\nwikipedia\n\n\n...\n...\n...\n...\n...\n\n\n1852\nStatistics\nhttps://stats.wikimedia.org/#/en.wikipedia.org\nTrue\nwikimedia\n\n\n1853\nCookie statement\nhttps://foundation.wikimedia.org/wiki/Special:...\nTrue\nwikimedia\n\n\n1854\nMobile view\nhttps://en.wikipedia.org//en.m.wikipedia.org/w...\nFalse\nwikipedia\n\n\n1855\n\nhttps://wikimediafoundation.org/\nTrue\nwikimediafoundation\n\n\n1856\n\nhttps://www.mediawiki.org\nTrue\nmediawiki\n\n\n\n\n1857 rows × 4 columns\n\n\n\nFinally let’s count the number of times each primary domain appears and print any that appear twice or more.\n\nprimary_domain_counts = link_df.value_counts('primary_domain')\nprimary_domain_counts = primary_domain_counts.sort_values(ascending=False)\nprimary_domain_counts = primary_domain_counts.reset_index() \n\nprimary_domain_counts[primary_domain_counts['count'] &gt;= 2]\n\n\n\n\n\n\n\n\nprimary_domain\ncount\n\n\n\n\n0\nwikipedia\n1745\n\n\n1\nworldhappiness\n28\n\n\n2\narchive\n12\n\n\n3\nwikimedia\n7\n\n\n4\nun\n6\n\n\n5\nac\n4\n\n\n6\nnytimes\n3\n\n\n7\nnih\n3\n\n\n8\ngallup\n3\n\n\n9\ndoi\n2\n\n\n10\noecd\n2\n\n\n11\ncnn\n2\n\n\n12\ntheguardian\n2\n\n\n13\nunsdsn\n2\n\n\n14\nresearchgate\n2\n\n\n15\ngrossnationalhappiness\n2\n\n\n16\ngov\n2\n\n\n17\nwikidata\n2"
  },
  {
    "objectID": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#extracting-tables",
    "href": "posts/2024-GESIS-2-1-obtaining-data-scraping.html#extracting-tables",
    "title": "Web Scraping 101",
    "section": "Extracting Tables",
    "text": "Extracting Tables\nNext, let’s learn how to extract and process table data from a static website.\n\nFinding and Parsing Tables with BeautifulSoup and Pandas\nWe’ll start by locating all tables within the HTML document. As you might expect, BeautifulSoup allows us to search for all table tags in the HTML.2\n2 In some cases, it may be easier to skip BeautifulSoup entirely when working with tables and instead pass the HTML directly to Pandas. This approach is particularly useful when you’re confident that the page contains well-structured tables. If you run tables = pd.read_html(StringIO(response.text)), Pandas will return a list of dataframes corresponding to each table on the page.\ntables = soup.find_all('table')\nprint(f'Found {len(tables)} table(s)')\n\nFound 34 table(s)\n\n\nAt this point, we’ve identified how many tables are present on the page – 34 – but we aren’t interested in all of them. Let’s take a look at what we have and make a decision about how to proceed.\n\nfor index, table in enumerate(tables):\n    headers = []\n    for th in table.find_all('th'):\n        headers.append(th.get_text(strip=True))\n    print(f\"Table {index+1} Column Names: {headers}\")\n\nTable 1 Column Names: ['Country rankings for the under-30s', 'Overall rank', 'Country or region', 'Life evaluation']\nTable 2 Column Names: ['Overall rank', 'Country or region', 'Life evaluation']\nTable 3 Column Names: ['Descriptions']\nTable 4 Column Names: ['Descriptions']\nTable 5 Column Names: ['Descriptions']\nTable 6 Column Names: ['Descriptions']\nTable 7 Column Names: ['Descriptions']\nTable 8 Column Names: ['Descriptions']\nTable 9 Column Names: ['Descriptions']\nTable 10 Column Names: ['Descriptions']\nTable 11 Column Names: ['Descriptions']\nTable 12 Column Names: ['Descriptions']\nTable 13 Column Names: ['Descriptions']\nTable 14 Column Names: ['Descriptions']\nTable 15 Column Names: ['Table', 'Overall rank', 'Country or region', 'Life evaluation']\nTable 16 Column Names: ['Overall rank', 'Country or region', 'Life evaluation']\nTable 17 Column Names: ['Table', 'Overall rank', 'Country or region']\nTable 18 Column Names: ['Overall rank', 'Country or region']\nTable 19 Column Names: ['Table', 'Overall rank', 'Country or region']\nTable 20 Column Names: ['Overall rank', 'Country or region']\nTable 21 Column Names: ['Table', 'Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 22 Column Names: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 23 Column Names: ['Table', 'Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 24 Column Names: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 25 Column Names: ['Table', 'Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 26 Column Names: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nTable 27 Column Names: ['Table', 'Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual']\nTable 28 Column Names: ['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual']\nTable 29 Column Names: ['Table', 'Overall Rank[61][62]', 'Country', 'Score', 'Change OverPrior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust']\nTable 30 Column Names: []\nTable 31 Column Names: ['Overall Rank[61][62]', 'Country', 'Score', 'Change OverPrior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust']\nTable 32 Column Names: ['Table', 'Rank[64]', 'Country', 'Happiness']\nTable 33 Column Names: ['Rank[64]', 'Country', 'Happiness']\nTable 34 Column Names: ['vteLists of countriesbyquality of liferankings', 'General', 'Economic', 'Environment', 'Health', 'Social/political']\n\n\nThis loop extracts the headers from each table, which can help us identify the tables we want.3 Suppose we’re looking for a table that contains information related to “Freedom to make life choices” or other specific indicators. We can easily find tables that contain the relevant column.\n3 You may notice that I’m using the strip argument for .get_text() this time. I do find it useful in situations like these, where I want to see the column names (which rarely contain spaces) without any additional characters (such as \\n, etc.).\ntables_filtered = [\n    table \n    for table in tables \n    if \"Freedom to make life choices\" in table.get_text(strip=True)\n]\n\nlen(tables_filtered)\n\n11\n\n\nNow that we’ve filtered the relevant tables, we can convert them into a format that’s easier to work with, such as a Pandas DataFrame.\n\ndfs = pd.read_html(StringIO(str(tables_filtered)))\nlen(dfs)\n\n17\n\n\nPandas’ read_html() function is quite powerful and can automatically extract tables from HTML content. However, sometimes it might pick up more than we expect. In this case, it found an additional 6 tables in the same HTML string!\nLet’s explore the extracted dataframes to see what we’ve got.\n\nfor df in dfs:\n    print(list(df.columns))\n\n['Table', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n['Table', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n['Table', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8']\n['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8']\n['Table', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11']\n['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual']\n['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual']\n['Table', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9']\n[0, 1]\n['Overall Rank [61][62]', 'Country', 'Score', 'Change Over Prior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust']\n[0, 1]\n['Overall Rank [61][62]', 'Country', 'Score', 'Change Over Prior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust']\n\n\n\n\nCleaning Up Extracted Tables\nWe can see that some of our dataframes have columns that are Unnamed. This generally indicates that the table contains multiple headers; we’ll need to inspect the dataframes to see what’s going on. The first dataframe illustrates the problem:\n\nexample_df = dfs[0]\nexample_df.head()\n\n\n\n\n\n\n\n\nTable\nUnnamed: 1\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\n\n\n\n\n0\nOverall rank Country or region Score GDP per c...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nOverall rank\nCountry or region\nScore\nGDP per capita\nSocial support\nHealthy life expectancy\nFreedom to make life choices\nGenerosity\nPerceptions of corruption\n\n\n2\n1\nFinland\n7.809\n1.285\n1.500\n0.961\n0.662\n0.160\n0.478\n\n\n3\n2\nDenmark\n7.646\n1.327\n1.503\n0.979\n0.665\n0.243\n0.495\n\n\n4\n3\nSwitzerland\n7.560\n1.391\n1.472\n1.041\n0.629\n0.269\n0.408\n\n\n\n\n\n\n\nIf we just had one table, such as the one we’re looking at here, we could easily clean it up by setting the second row as the header and dropping the irrelevant rows.\n\nexample_df.columns = example_df.iloc[1]  \nexample_df.drop([0, 1], inplace=True)  \nexample_df = example_df.reset_index()\nexample_df.head()\n\n\n\n\n\n\n\n1\nindex\nOverall rank\nCountry or region\nScore\nGDP per capita\nSocial support\nHealthy life expectancy\nFreedom to make life choices\nGenerosity\nPerceptions of corruption\n\n\n\n\n0\n2\n1\nFinland\n7.809\n1.285\n1.500\n0.961\n0.662\n0.160\n0.478\n\n\n1\n3\n2\nDenmark\n7.646\n1.327\n1.503\n0.979\n0.665\n0.243\n0.495\n\n\n2\n4\n3\nSwitzerland\n7.560\n1.391\n1.472\n1.041\n0.629\n0.269\n0.408\n\n\n3\n5\n4\nIceland\n7.504\n1.327\n1.548\n1.001\n0.662\n0.362\n0.145\n\n\n4\n6\n5\nNorway\n7.488\n1.424\n1.495\n1.008\n0.670\n0.288\n0.434\n\n\n\n\n\n\n\nThis looks much better, but… we aren’t ready to do this for all dataframes just yet. As part of diagnosing the problem, we can spend a bit more time inspecting the page’s source code using our web browser’s developer tools. This reveals the underlying source of the problem: these tables are nested! The tables we want are nested inside tables we don’t care about. How do we get them out?\nTo address this problem, we can update our approach to identify the outer tables and then, if and when it finds one, look for an inner table. Let’s check the logic of this approach before implementing it.\n\n# Find all the outer tables\nouter_tables = soup.find_all('table')\n\n# Loop through all outer tables\nfor outer_table in outer_tables:\n    # Try to find a nested table inside the outer table\n    inner_table = outer_table.find('table')\n\n    if inner_table:\n        # Convert the inner table to a DataFrame\n        table_df = pd.read_html(StringIO(str(inner_table)))[0]\n        print(table_df.head(3), '\\n')\n\n   Overall rank Country or region  Life evaluation\n0             1         Lithuania            7.795\n1             2            Israel            7.667\n2             3           Iceland            7.658 \n\n   Overall rank Country or region  Life evaluation\n0             1           Finland            7.741\n1             2           Denmark            7.583\n2             3           Iceland            7.525 \n\n   Overall rank Country or region\n0             1             India\n1             2           Denmark\n2             3           Iceland \n\n   Overall rank Country or region\n0             1           Finland\n1             2           Denmark\n2             3           Iceland \n\n   Overall rank Country or region  Score  GDP per capita  Social support  \\\n0             1           Finland  7.809           1.285           1.500   \n1             2           Denmark  7.646           1.327           1.503   \n2             3       Switzerland  7.560           1.391           1.472   \n\n   Healthy life expectancy  Freedom to make life choices  Generosity  \\\n0                    0.961                         0.662       0.160   \n1                    0.979                         0.665       0.243   \n2                    1.041                         0.629       0.269   \n\n   Perceptions of corruption  \n0                      0.478  \n1                      0.495  \n2                      0.408   \n\n   Overall rank Country or region  Score  GDP per capita  Social support  \\\n0             1           Finland  7.769           1.340           1.587   \n1             2           Denmark  7.600           1.383           1.573   \n2             3            Norway  7.554           1.488           1.582   \n\n   Healthy life expectancy  Freedom to make life choices  Generosity  \\\n0                    0.986                         0.596       0.153   \n1                    0.996                         0.592       0.252   \n2                    1.028                         0.603       0.271   \n\n   Perceptions of corruption  \n0                      0.393  \n1                      0.410  \n2                      0.341   \n\n   Overall rank Country or region  Score  GDP per capita  Social support  \\\n0             1           Finland  7.632           1.305           1.592   \n1             2            Norway  7.594           1.456           1.582   \n2             3           Denmark  7.555           1.351           1.590   \n\n   Freedom to make life choices  Generosity  Perceptions of corruption  \\\n0                         0.874       0.681                      0.202   \n1                         0.861       0.686                      0.286   \n2                         0.868       0.683                      0.284   \n\n   Unnamed: 8  \n0       0.393  \n1       0.340  \n2       0.408   \n\n  Overall Rank Change in rank  Country  Score  Change in score  \\\n0            1              3   Norway  7.537            0.039   \n1            2              1  Denmark  7.522            0.004   \n2            3            NaN  Iceland  7.504            0.003   \n\n   GDP per capita  Social support  Healthy life expectancy  \\\n0           1.616           1.534                    0.797   \n1           1.482           1.551                    0.793   \n2           1.481           1.611                    0.834   \n\n   Freedom to make life choices  Generosity  Trust  Residual  \n0                         0.635       0.362  0.316     2.277  \n1                         0.626       0.355  0.401     2.314  \n2                         0.627       0.476  0.154     2.323   \n\n                                                   0  \\\n0  Explained by: GDP per capita  Explained by: So...   \n\n                                                   1  \n0  Explained by: Freedom to make life choices  Ex...   \n\n   Rank[64]      Country  Happiness\n0         1      Denmark      7.693\n1         2       Norway      7.655\n2         3  Switzerland      7.650 \n\n\n\nIt appears that finding the inner table gets us the data we want. Before we implement this solution, let’s do one more important bit of data processing: associating tables with the dates of the reports they came from.\n\n\nAssociating Tables with Dates\nThe main data processing we want to do here is associate each table with the corresponding report year. By inspecting the HTML, we can identify the year each table belongs to by looking at the headers.\nOur goal is to extract tables from the webpage and associate each with the publication year of the report the data come from, and we will do that by using information in the section headers preceding the tables we collect. In other words, we want to capture the year from h3 headers and link it to the subsequent table element. We also know that some tables might contain nested tables, and that the data we are interested in is stored in the inner table. We want our code to extract and process these inner tables as needed.\nOur code is getting a bit more complex here. If you’re new to Python, it’s helps to know exactly what we are trying to do and why. Here it is, in brief:\n\nIdentify the year. We can do this by looking for h3 headers that contain a year (formatted as YYYY) followed by the word ‘report.’ We’ll use a regular expression to identify the pattern. We’ll keep track of the current year as we move through the document.\nAssociate tables with years. After identifying a year, we check for the next table element. If a table is found, we associate it with the most recently identified year.\nHandle Nested Tables. We now know that some tables contain nested tables, and that the data we want is in the inner table. If a nested table is found, we’ll keep and process the inner table and discard the outer table.\n\n\n# Initialize variables\ntables_with_years = []\nyear_pattern = re.compile(r'(\\d{4}) report')  # Regex pattern to identify years\ncurrent_year = None\n\n# Loop through all elements, capturing headings and tables\nfor element in soup.find_all(['h3', 'table']):\n    # Check if the element is an h3 heading containing the year pattern\n    if element.name == 'h3':\n        header_text = element.get_text(strip=True)\n        match = year_pattern.search(header_text)\n        if match:\n            current_year = match.group(1)  # Track the year\n\n    # If the element is a table, associate it with the current year\n    elif element.name == 'table' and current_year:\n        # Convert the outer table element to a DataFrame\n        outer_html_string = str(element)\n        outer_table_df = pd.read_html(StringIO(outer_html_string))[0] \n        \n        # Check for a nested table within the outer table\n        inner_table = element.find('table')\n        if inner_table:\n            # Convert the inner table to a DataFrame\n            inner_html_string = str(inner_table)\n            inner_table_df = pd.read_html(StringIO(inner_html_string))[0]\n            inner_table_df['Year'] = current_year\n            tables_with_years.append(inner_table_df)\n        else:\n            # If no nested table, add the outer table DataFrame\n            outer_table_df['Year'] = current_year\n            tables_with_years.append(outer_table_df)\n\n# Check the number of tables collected\nlen(tables_with_years)\n\n20\n\n\nNow that we’ve collected the tables and associated them with their respective years, let’s inspect them to ensure the data was extracted correctly. We’ll loop through the list of tables and check their column names.\n\nfor i, table in enumerate(tables_with_years):\n    print(f\"Table {i} columns:\", list(table.columns))\n\nTable 0 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 1 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 2 columns: ['Overall rank', 'Country or region', 'Year']\nTable 3 columns: ['Overall rank', 'Country or region', 'Year']\nTable 4 columns: ['Overall rank', 'Country or region', 'Year']\nTable 5 columns: ['Overall rank', 'Country or region', 'Year']\nTable 6 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 7 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 8 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 9 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 10 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 11 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 12 columns: ['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 13 columns: ['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 14 columns: [0, 1, 'Year']\nTable 15 columns: [0, 1, 'Year']\nTable 16 columns: ['Overall Rank [61][62]', 'Country', 'Score', 'Change Over Prior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Year']\nTable 17 columns: ['Rank[64]', 'Country', 'Happiness', 'Year']\nTable 18 columns: ['Rank[64]', 'Country', 'Happiness', 'Year']\nTable 19 columns: ['vteLists of countries by quality of life rankings', 'vteLists of countries by quality of life rankings.1', 'Year']\n\n\nMost of these tables look good, although there are two tables that look like they may need some additional work (14 and 15), and we seem to have some duplicate tables.\n\ntables_with_years[14].head()\n\n\n\n\n\n\n\n\n0\n1\nYear\n\n\n\n\n0\nExplained by: GDP per capita Explained by: So...\nExplained by: Freedom to make life choices Ex...\n2016\n\n\n\n\n\n\n\n\ntables_with_years[15].head()\n\n\n\n\n\n\n\n\n0\n1\nYear\n\n\n\n\n0\nExplained by: GDP per capita Explained by: So...\nExplained by: Freedom to make life choices Ex...\n2016\n\n\n\n\n\n\n\nWe’ll set tables 14 and 15 aside for now.\n\ndel tables_with_years[15] # higher index first!\ndel tables_with_years[14]\n\nfor i, table in enumerate(tables_with_years):\n    print(f\"Table {i} columns:\", list(table.columns))\n\nTable 0 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 1 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 2 columns: ['Overall rank', 'Country or region', 'Year']\nTable 3 columns: ['Overall rank', 'Country or region', 'Year']\nTable 4 columns: ['Overall rank', 'Country or region', 'Year']\nTable 5 columns: ['Overall rank', 'Country or region', 'Year']\nTable 6 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 7 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 8 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 9 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 10 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 11 columns: ['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 12 columns: ['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 13 columns: ['Overall Rank', 'Change in rank', 'Country', 'Score', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 14 columns: ['Overall Rank [61][62]', 'Country', 'Score', 'Change Over Prior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Year']\nTable 15 columns: ['Rank[64]', 'Country', 'Happiness', 'Year']\nTable 16 columns: ['Rank[64]', 'Country', 'Happiness', 'Year']\nTable 17 columns: ['vteLists of countries by quality of life rankings', 'vteLists of countries by quality of life rankings.1', 'Year']\n\n\nWhat about the potential duplicate tables? Let’s take a look.\n\ntables_with_years[0].head()\n\n\n\n\n\n\n\n\nOverall rank\nCountry or region\nLife evaluation\nYear\n\n\n\n\n0\n1\nFinland\n7.741\n2024\n\n\n1\n2\nDenmark\n7.583\n2024\n\n\n2\n3\nIceland\n7.525\n2024\n\n\n3\n4\nSweden\n7.344\n2024\n\n\n4\n5\nIsrael\n7.341\n2024\n\n\n\n\n\n\n\n\ntables_with_years[1].head()\n\n\n\n\n\n\n\n\nOverall rank\nCountry or region\nLife evaluation\nYear\n\n\n\n\n0\n1\nFinland\n7.741\n2024\n\n\n1\n2\nDenmark\n7.583\n2024\n\n\n2\n3\nIceland\n7.525\n2024\n\n\n3\n4\nSweden\n7.344\n2024\n\n\n4\n5\nIsrael\n7.341\n2024\n\n\n\n\n\n\n\n\ntables_with_years[0] == tables_with_years[1]\n\n\n\n\n\n\n\n\nOverall rank\nCountry or region\nLife evaluation\nYear\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nTrue\nTrue\n\n\n2\nTrue\nTrue\nTrue\nTrue\n\n\n3\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nTrue\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n138\nTrue\nTrue\nTrue\nTrue\n\n\n139\nTrue\nTrue\nTrue\nTrue\n\n\n140\nTrue\nTrue\nTrue\nTrue\n\n\n141\nTrue\nTrue\nTrue\nTrue\n\n\n142\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n143 rows × 4 columns\n\n\n\nIt looks like we do have duplicated data. There are a few ways we can deal with this, but the easiest way is to leave things as they are and proceed. Once we’ve got everything in one clean dataframe, we can solve our duplicate data problem by dropping duplicate rows.\nOur next steps, then, are as follows:\n\nselect the final dataframes,\nalign their column names,\nconcatenate them into one master dataframe, and\ndrop duplicate rows.\n\nLet’s select the dataframes with a “Life evaluation” column.4 It appears that only two of our dataframes contain this data, but that’s due to inconsistent naming; the “Life evaluation” score is simply labelled “Score” in some dataframes. Additionally, some dataframes have a column called ‘Country or region’ and others have “Country.” Let’s align these column names.\n4 Gallupe’s Life evaluation index is a measure of subjective wellbeing based on how people rate their current and expected future lives. Gallupe asks people to pick a number between 0 and 10 where 0 represents their worst possible life and 10 represents their best possible life. Participants say where they feel they are now, and where they thing they will be in 5 years. You can learn a bit more about the index from Gallupe, or by browsing the World Happiness Reports here.\nnew_columns = {\n  \"Country\": \"Country or region\",\n  \"Score\": \"Life evaluation\"\n}\n\nfor df in tables_with_years:\n    df.rename(columns=new_columns, inplace=True)\n\n\nfor i, table in enumerate(tables_with_years):\n    print(f\"Table {i} columns:\", list(table.columns))\n\nTable 0 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 1 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'Year']\nTable 2 columns: ['Overall rank', 'Country or region', 'Year']\nTable 3 columns: ['Overall rank', 'Country or region', 'Year']\nTable 4 columns: ['Overall rank', 'Country or region', 'Year']\nTable 5 columns: ['Overall rank', 'Country or region', 'Year']\nTable 6 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 7 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 8 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 9 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Year']\nTable 10 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 11 columns: ['Overall rank', 'Country or region', 'Life evaluation', 'GDP per capita', 'Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption', 'Unnamed: 8', 'Year']\nTable 12 columns: ['Overall Rank', 'Change in rank', 'Country or region', 'Life evaluation', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 13 columns: ['Overall Rank', 'Change in rank', 'Country or region', 'Life evaluation', 'Change in score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Residual', 'Year']\nTable 14 columns: ['Overall Rank [61][62]', 'Country or region', 'Life evaluation', 'Change Over Prior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust', 'Year']\nTable 15 columns: ['Rank[64]', 'Country or region', 'Happiness', 'Year']\nTable 16 columns: ['Rank[64]', 'Country or region', 'Happiness', 'Year']\nTable 17 columns: ['vteLists of countries by quality of life rankings', 'vteLists of countries by quality of life rankings.1', 'Year']\n\n\n\nfinal_dfs = []\nfor df in tables_with_years:\n    if 'Life evaluation' in df.columns:\n        final_dfs.append(df[['Country or region', \"Year\", \"Life evaluation\"]])\n\nfinal_df = pd.concat(final_dfs)\nfinal_df.drop_duplicates(inplace=True)\nfinal_df.dropna(inplace=True)\nfinal_df = final_df.reset_index()\nfinal_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 922 entries, 0 to 921\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   index              922 non-null    int64 \n 1   Country or region  922 non-null    object\n 2   Year               922 non-null    object\n 3   Life evaluation    922 non-null    object\ndtypes: int64(1), object(3)\nmemory usage: 28.9+ KB\n\n\nIt looks good, but there’s actually a problem lurking in this dataframe. If you inspect the output from info(), you’ll see that Life evaluation is actually an object / string, not a float. If we convert it to a float, most of our data gets converted to NaNs! 😲\n\nwill_have_nans = final_df.copy()\nwill_have_nans['Life evaluation'] = will_have_nans['Life evaluation'].str.strip()\n\n# Try to reapply the extraction and conversion\nwill_have_nans['Life evaluation'] = will_have_nans['Life evaluation'].str.extract(r'([\\d\\.]+)')\nwill_have_nans['Life evaluation'] = pd.to_numeric(will_have_nans['Life evaluation'], errors='coerce')\nwill_have_nans.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 922 entries, 0 to 921\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   index              922 non-null    int64  \n 1   Country or region  922 non-null    object \n 2   Year               922 non-null    object \n 3   Life evaluation    157 non-null    float64\ndtypes: float64(1), int64(1), object(2)\nmemory usage: 28.9+ KB\n\n\nWhat’s going on here!\nIf you print the unique values, you’ll see that Python recognizes some observations as floats while others are floats that Python thinks are strings. And then there is one observation that we can be sure is causing trouble: 5.305[b].\n\nprint(final_df['Life evaluation'].unique())\n\n[7.741 7.583 7.525 7.344 7.341 7.319 7.302 7.122 7.06 7.057 7.029 6.955\n 6.951 6.905 6.9 6.894 6.838 6.822 6.818 6.749 6.743 6.733 6.725 6.719\n 6.678 6.611 6.609 6.594 6.561 6.523 6.503 6.491 6.469 6.448 6.442 6.421\n 6.411 6.36 6.358 6.346 6.324 6.287 6.284 6.272 6.257 6.234 6.195 6.188\n 6.068 6.06 6.058 6.048 6.043 6.03 6.017 5.977 5.976 5.975 5.973 5.968\n 5.959 5.942 5.934 5.877 5.866 5.842 5.841 5.823 5.816 5.785 5.784 5.725\n 5.714 5.707 5.696 5.695 5.607 5.568 5.463 5.455 5.422 5.369 5.364 5.316\n 5.304 5.281 5.221 5.216 5.185 5.166 5.158 5.139 5.106 5.08 5.023 4.975\n 4.969 4.923 4.893 4.881 4.879 4.874 4.873 4.832 4.795 4.657 4.556 4.548\n 4.505 4.485 4.471 4.47 4.422 4.377 4.372 4.354 4.341 4.289 4.269 4.232\n 4.228 4.214 4.186 4.054 3.977 3.898 3.886 3.861 3.781 3.566 3.561 3.502\n 3.421 3.383 3.341 3.295 3.245 3.186 2.707 1.721 7.809 7.646 7.56 7.504\n 7.488 7.449 7.353 7.3 7.294 7.238 7.232 7.223 7.165 7.129 7.121 7.094\n 7.076 6.94 6.911 6.864 6.791 6.773 6.664 6.465 6.455 6.44 6.406 6.401\n 6.399 6.387 6.377 6.376 6.363 6.348 6.325 6.305 6.281 6.258 6.228 6.227\n 6.215 6.192 6.186 6.163 6.159 6.137 6.124 6.102 6.101 6.022 6.006 6.0\n 5.999 5.953 5.95 5.925 5.911 5.89 5.872 5.871 5.797 5.778 5.747 5.693\n 5.692 5.689 5.674 5.608 5.556 5.546 5.542 5.54 5.536 5.515 5.51 5.505\n 5.489 5.456 5.384 5.353 5.286 5.233 5.198 5.194 5.165 5.16 5.148 5.137\n 5.132 5.124 5.119 5.102 5.095 5.085 5.053 5.005 4.981 4.949 4.91 4.889\n 4.883 4.848 4.833 4.829 4.814 4.785 4.772 4.769 4.751 4.729 4.724 4.677\n 4.673 4.672 4.633 4.624 4.583 4.571 4.561 4.558 4.553 4.432 4.423 4.392\n 4.375 4.327 4.311 4.308 4.187 4.166 4.151 3.926 3.775 3.759 3.721 3.653\n 3.573 3.538 3.527 3.479 3.476 3.312 3.299 2.817 2.567 7.769 7.6 7.554\n 7.494 7.48 7.343 7.307 7.278 7.246 7.228 7.167 7.139 7.09 7.054 7.021\n 6.985 6.923 6.892 6.852 6.825 6.726 6.595 6.592 6.446 6.444 6.436 6.375\n 6.374 6.354 6.321 6.3 6.293 6.262 6.253 6.223 6.199 6.198 6.182 6.174\n 6.149 6.125 6.118 6.105 6.1 6.086 6.07 6.046 6.028 6.011 6.008 5.94 5.895\n 5.893 5.888 5.886 5.86 5.809 5.779 5.758 5.743 5.718 5.697 5.653 5.648\n 5.631 5.603 5.529 5.525 5.523 5.467 5.432 5.43 5.425 5.386 5.373 5.339\n 5.323 5.287 5.285 5.274 5.265 5.261 5.247 5.211 5.208 5.197 5.192 5.191\n 5.175 5.082 5.044 5.011 4.996 4.944 4.913 4.906 4.812 4.799 4.796 4.722\n 4.719 4.707 4.7 4.696 4.681 4.668 4.639 4.628 4.587 4.559 4.534 4.519\n 4.516 4.509 4.49 4.466 4.461 4.456 4.437 4.418 4.39 4.374 4.366 4.36 4.35\n 4.332 4.286 4.212 4.189 4.107 4.085 4.015 3.975 3.973 3.933 3.802 3.663\n 3.597 3.488 3.462 3.41 3.38 3.334 3.231 3.203 3.083 2.853 7.632 7.594\n 7.555 7.495 7.487 7.441 7.328 7.324 7.314 7.272 7.19 7.072 6.977 6.965\n 6.927 6.91 6.886 6.814 6.774 6.711 6.627 6.489 6.488 6.476 6.441 6.43\n 6.419 6.388 6.382 6.379 6.371 6.343 6.322 6.31 6.26 6.173 6.167 6.141\n 6.123 6.096 6.083 6.072 5.956 5.952 5.948 5.945 5.933 5.915 5.891 5.875\n 5.835 5.81 5.79 5.762 5.752 5.739 5.681 5.663 5.662 5.64 5.636 5.62 5.566\n 5.524 5.504 5.483 5.472 5.41 5.398 5.358 5.347 5.321 5.302 5.295 5.254\n 5.246 5.201 5.199 5.161 5.155 5.131 5.129 5.125 5.103 5.093 4.982 4.933\n 4.88 4.806 4.758 4.743 4.671 4.631 4.623 4.592 4.586 4.5 4.447 4.441\n 4.433 4.424 4.419 4.417 4.41 4.356 4.34 4.321 4.301 4.245 4.19 4.161\n 4.141 4.139 4.103 3.999 3.964 3.808 3.795 3.774 3.692 3.632 3.59 3.587\n 3.582 3.495 3.408 3.355 3.303 3.254 2.905 '7.537' '7.522' '7.504' '7.494'\n '7.469' '7.377' '7.316' '7.314' '7.284' '7.213' '7.079' '7.006' '6.993'\n '6.977' '6.951' '6.891' '6.863' '6.714' '6.652' '6.648' '6.635' '6.609'\n '6.599' '6.578' '6.572' '6.527' '6.454' '6.452' '6.442' '6.424' '6.422'\n '6.403' '6.375' '6.357' '6.344' '6.168' '6.105' '6.098' '6.087' '6.084'\n '6.080' '6.071' '6.008' '6.003' '5.973' '5.971' '5.964' '5.963' '5.956'\n '5.920' '5.902' '5.872' '5.850' '5.838' '5.825' '5.823' '5.822' '5.819'\n '5.810' '5.758' '5.715' '5.629' '5.621' '5.611' '5.569' '5.525' '5.500'\n '5.493' '5.472' '5.430' '5.395' '5.336' '5.324' '5.311' '5.305[b]'\n '5.293' '5.279' '5.273' '5.269' '5.262' '5.250' '5.237' '5.235' '7.342'\n '5.230' '5.227' '5.225' '5.195' '5.182' '5.181' '5.175' '5.151' '5.074'\n '5.041' '5.011' '5.004' '4.962' '4.955' '4.829' '4.805' '4.775' '4.735'\n '4.714' '4.709' '4.695' '4.692' '4.644' '4.608' '4.574' '4.553' '4.550'\n '4.545' '4.535' '4.514' '4.497' '4.465' '4.460' '4.440' '4.376' '4.315'\n '4.292' '4.291' '4.286' '4.280' '4.190' '4.180' '4.168' '4.139' '4.120'\n '4.096' '4.081' '4.032' '4.028' '3.970' '3.936' '3.875' '3.808' '3.795'\n '3.794' '3.766' '3.657' '3.644' '3.603' '3.593' '3.591' '3.533' '3.507'\n '3.495' '3.471' '3.462' '3.349' '2.905' '2.693' 7.526 7.509 7.501 7.498\n 7.413 7.404 7.339 7.334 7.313 7.291 7.267 7.119 7.104 7.087 7.039 6.994\n 6.952 6.929 6.907 6.871 6.778 6.739 6.705 6.701 6.65 6.596 6.573 6.545\n 6.481 6.478 6.474 6.361 6.355 6.269 6.239 6.218 6.168 6.084 6.078 6.005\n 5.992 5.987 5.921 5.919 5.897 5.856 5.822 5.813 5.802 5.771 5.768 5.658\n 5.615 5.56 5.538 5.528 5.517 5.488 5.458 5.44 5.401 5.389 5.314 5.303\n 5.291 5.279 5.245 5.196 5.177 5.163 5.151 5.145 5.123 5.121 5.061 5.057\n 5.045 5.033 4.907 4.876 4.875 4.871 4.813 4.793 4.754 4.655 4.643 4.635\n 4.575 4.574 4.513 4.508 4.459 4.415 4.404 4.395 4.362 4.324 4.276 4.272\n 4.252 4.236 4.219 4.217 4.201 4.193 4.156 4.121 4.073 4.028 3.974 3.956\n 3.916 3.907 3.866 3.856 3.832 3.763 3.739 3.724 3.695 3.666 3.622 3.607\n 3.515 3.484 3.36 3.069]\n\n\nIf we handle this situation carefully, we’ll get back the data we expect. To target our problematic observation(s), we’ll temporarily create a list, iterate over it to handle each observation as it needs to be handled, and then add the clean values back to our dataframe.\n\nvals = final_df['Life evaluation'].tolist()\nlen(vals)\n\n922\n\n\nAs we process these values, we’ll check their type. If it’s already a float, we’ll do nothing. If it’s a string, we’ll target the problem we know about and then attempt to convert it to a float. Then we’ll check the outcome.\n\ncleaned = []\n\nfor val in vals:  \n    if isinstance(val, float):  # Check if the value is already a float\n        cleaned.append(val)\n    elif isinstance(val, str):  # Check if the value is a string\n        cleaned.append(float(val.replace('[b]', '')))  # Remove '[b]' and convert to float\n    else:\n        print(f\"Unexpected type: {type(val)}\")  # Handle unexpected types\n\nfinal_df['Life evaluation'] = pd.Series(cleaned)\nfinal_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 922 entries, 0 to 921\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   index              922 non-null    int64  \n 1   Country or region  922 non-null    object \n 2   Year               922 non-null    object \n 3   Life evaluation    922 non-null    float64\ndtypes: float64(1), int64(1), object(2)\nmemory usage: 28.9+ KB\n\n\nWe now have a float without missing values! 🔥😎\n\nfinal_df.sort_values('Life evaluation', ascending=False, inplace=True)\nfinal_df.head(30)\n\n\n\n\n\n\n\n\nindex\nCountry or region\nYear\nLife evaluation\n\n\n\n\n143\n0\nFinland\n2020\n7.809\n\n\n296\n0\nFinland\n2019\n7.769\n\n\n0\n0\nFinland\n2024\n7.741\n\n\n144\n1\nDenmark\n2020\n7.646\n\n\n452\n0\nFinland\n2018\n7.632\n\n\n297\n1\nDenmark\n2019\n7.600\n\n\n453\n1\nNorway\n2018\n7.594\n\n\n1\n1\nDenmark\n2024\n7.583\n\n\n145\n2\nSwitzerland\n2020\n7.560\n\n\n454\n2\nDenmark\n2018\n7.555\n\n\n298\n2\nNorway\n2019\n7.554\n\n\n608\n0\nNorway\n2017\n7.537\n\n\n765\n0\nDenmark\n2016\n7.526\n\n\n2\n2\nIceland\n2024\n7.525\n\n\n609\n1\nDenmark\n2017\n7.522\n\n\n766\n1\nSwitzerland\n2016\n7.509\n\n\n610\n2\nIceland\n2017\n7.504\n\n\n146\n3\nIceland\n2020\n7.504\n\n\n767\n2\nIceland\n2016\n7.501\n\n\n768\n3\nNorway\n2016\n7.498\n\n\n455\n3\nIceland\n2018\n7.495\n\n\n611\n3\nSwitzerland\n2017\n7.494\n\n\n299\n3\nIceland\n2019\n7.494\n\n\n147\n4\nNorway\n2020\n7.488\n\n\n300\n4\nNetherlands\n2019\n7.488\n\n\n456\n4\nSwitzerland\n2018\n7.487\n\n\n301\n5\nSwitzerland\n2019\n7.480\n\n\n612\n4\nFinland\n2017\n7.469\n\n\n148\n5\nNetherlands\n2020\n7.449\n\n\n457\n5\nNetherlands\n2018\n7.441\n\n\n\n\n\n\n\n\n\nVisualization\nBefore we move on to an example of collecting data from an API, let’s create a visualization of the data we just scraped and processed. We’re going to create a small multiples plot. The code below looks a bit complex, but it’s just calculating the size of the figure needed and then looping over each country to create a plot.\nsorted_data = final_df.sort_values(['Country or region', 'Year'])\nsorted_data = sorted_data[sorted_data['Country or region'] != \"World\"]\ncountries = sorted_data['Country or region'].unique()\n\n# Define the number of rows and columns for the subplots\nn_cols = 4  # You can adjust this based on how many plots you want per row\nn_rows = len(countries) // n_cols + (len(countries) % n_cols &gt; 0)\n\n# Create the figure and subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2.5), constrained_layout=True, sharex=True, sharey=True)\n\n# Flatten the axes array for easier indexing\naxes = axes.flatten()\n\n# Loop through each country and create a subplot\nfor i, country in enumerate(countries):\n    country_data = sorted_data[sorted_data['Country or region'] == country]\n    axes[i].plot(country_data['Year'], country_data['Life evaluation'], marker='o')\n    axes[i].set_title(f'\\n{country}', fontsize=12)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].set_xlim(final_df['Year'].min(), final_df['Year'].max())\n    axes[i].set_ylim(0, final_df['Life evaluation'].max() + 1)\n    \n    axes[i].set_yticks(\n      np.arange(\n        int(final_df['Life evaluation'].min()), \n        int(final_df['Life evaluation'].max()) + 1, \n        1\n      )\n    )  \n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].grid(True)\n\n# Hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.savefig(\"output/happiness_report_small_multiples.png\", dpi=300)\n\n\n\nLooking pretty good!"
  },
  {
    "objectID": "posts/Quarto.html",
    "href": "posts/Quarto.html",
    "title": "How I built this site",
    "section": "",
    "text": "👋 Hi, this is just a test. Please come back later!"
  },
  {
    "objectID": "posts/2024-GESIS-3-1-text-analysis-foundations.html",
    "href": "posts/2024-GESIS-3-1-text-analysis-foundations.html",
    "title": "Text Analysis Foundations",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html",
    "href": "posts/2024-GESIS-5-1-abms.html",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "This notebook accompanies the recorded lecture on “Simulation and Agent-based Modelling.” It’s intended to provide you with a hands-on experience in building and analyzing agent-based models using Python.\n\n\nBy the end of this workshop, you will be able to:\n\nUnderstand the basic concepts and components of agent-based modeling.\nImplement a simple SIR model using Mesa, a Python library for agent-based modeling.\n\nIn this notebook, this is done via the course package icsspy, but the models we load here are the same ones I developed in the lecture.\n\nAnalyze the behavior of the SIR model under different conditions and parameters.\nExplore a Bounded Confidence Model to understand opinion dynamics.\nConduct sensitivity analysis and parameter sweeps to systematically investigate model behavior.\n\n\n\n\nLet’s start by importing the necessary packages and configuring our environment.\nimport mesa\nfrom mesa.batchrunner import batch_run\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport networkx as nx\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport icsspy\nicsspy.set_style()\nfrom icsspy.networks import plot_line_comparison\n\n\n\nWe’ll begin by exploring a classic Susceptible-Infected-Recovered (SIR) model. This model is often used to simulate the spread of infectious diseases.\n\n\nFirst, let’s load the SIRModel class from the course package and define the parameters for our first model, which simulates a disease with a high infection rate.\nfrom icsspy.abms.sir import SIRModel\n\nmodel_1_params = {'N': 100,\n 'grid_height': 40,\n 'grid_width': 40,\n 'infection_rate': 0.3,\n 'max_agent_step_size': 1,\n 'n_initial_infections': 2,\n 'n_iterations': 150,\n 'recovery_time_range': [14, 24]\n}\n\nprint(\"Model 1 Params\\n\", model_1_params)\nHere, we have a population of 100 agents on a 40x40 grid. The infection rate is set to 0.3, meaning there’s a 30% chance that a susceptible agent will become infected when in contact with an infected agent. The agents can move to adjacent cells, and their recovery time is randomly chosen between 14 and 24 time steps.\nNow, let’s run the model and store the results.\nmodel_1 = SIRModel(\n    n_initial_infections = model_1_params['n_initial_infections'],\n    grid_width = model_1_params['grid_width'],\n    grid_height = model_1_params['grid_height'],\n    N = model_1_params['N'],\n    infection_rate = model_1_params['infection_rate'],\n    recovery_time_range = model_1_params['recovery_time_range'],\n    max_agent_step_size = model_1_params['max_agent_step_size'],\n)\n\nfor i in range(model_1_params['n_iterations']):\n    model_1.step()\n\nm1res = model_1.datacollector.get_model_vars_dataframe()\nm1res\n\n\n\nmodel_2_params = {'N': 100,\n 'grid_height': 40,\n 'grid_width': 40,\n 'infection_rate': 0.15,\n 'max_agent_step_size': 1,\n 'n_initial_infections': 2,\n 'n_iterations': 150,\n 'recovery_time_range': [14, 24]\n}\n\nprint(\"\\nModel 2 Params\\n\", model_2_params)\nWe have kept most of the parameters the same as in Model 1, except for the infection rate, which is now 0.15, to simulate a less contagious disease.\nNow, let’s run Model 2.\nmodel_2 = SIRModel(\n    n_initial_infections = model_2_params['n_initial_infections'],\n    grid_width = model_2_params['grid_width'],\n    grid_height = model_2_params['grid_height'],\n    N = model_2_params['N'],\n    infection_rate = model_2_params['infection_rate'],\n    recovery_time_range = model_2_params['recovery_time_range'],\n    max_agent_step_size = model_2_params['max_agent_step_size'],\n)\n\nfor i in range(model_2_params['n_iterations']):\n    model_2.step()\n\nm2res = model_2.datacollector.get_model_vars_dataframe()\nm2res\n\n\n\nLet’s compare the infection curves from both models to see how the different infection rates affect the spread of the disease.\nfig, ax = plt.subplots()\nax.plot(m1res['Infected'], label=r'High transmissibility, $\\beta$=0.3')\nax.plot(m2res['Infected'], label=r'Low transmissibility $\\beta$=0.15')\nplt.xlabel(\"\\nDiscrete Steps in Time\")\nplt.ylabel(\"Proportion Infected\\n\")\nplt.legend(loc='upper right', fontsize=10)\nplt.savefig('output/sir_compare_models_1-2.png')\n\n\n\nIn addition to tracking the number of infected agents, it’s also useful to analyze how agents interact with each other during the simulation.\ninteraction_graphs = {}\ninteraction_graph_summaries = {}\n\nmodels = [model_1, model_2]\nfor i, model in enumerate(models, start=1):\n    wel = []\n    for agent in model.schedule.agents:\n        for k, v in agent.interactions.items():\n            wel.append((int(agent.unique_id), k, v))\n\n    G = nx.Graph()\n    G.add_weighted_edges_from(wel)\n    interaction_graphs[f'M{i}'] = G\n\n    avg_degree = round(sum(dict(G.degree()).values()) / float(G.number_of_nodes()), 2)\n    interaction_graph_summaries[f'M{i}'] = (G.number_of_nodes(), G.number_of_edges(), avg_degree)\ninteraction_graph_summaries = pd.DataFrame(interaction_graph_summaries).T\ninteraction_graph_summaries.columns = ['No. Nodes', 'No. Edges', 'Avg. Degree']\ninteraction_graph_summaries\n# Initialize an empty graph\nG = interaction_graphs['M2']\nweights = [d['weight'] for u, v, d in G.edges(data=True)]\n\nplt.figure(figsize=(8, 6))\nsns.ecdfplot(weights)\nplt.xlabel('Interaction Weight')\nplt.ylabel('ECDF')\nplt.title('ECDF of Interaction Weights')\nplt.grid(True)\nplt.savefig('output/compare_agent_networks.png', dpi=300)\n\n\n\n\nModel analysis is critical for understanding the behavior of an ABM and ensuring that it accurately reflects the real-world system being modeled. In this section, we’ll cover various analysis techniques, including descriptive, exploratory, comparative, and predictive analyses.\nSensitivity Analysis: Peak Proportion Infected Sensitivity analysis helps determine how changes in model parameters affect the outcomes. Here, we’ll focus on the peak proportion of infected agents as our outcome of interest.\nFirst, let’s set up a parameter sweep for the sensitivity analysis.\n\nparams = {\n    \"N\": 100,\n    \"grid_height\": 40,\n    \"grid_width\": 40,\n    \"infection_rate\": [0.2, 0.4, 0.6, 0.8,],\n    \"recovery_time_range\": [(7,14), (14,24)],\n    \"max_agent_step_size\": [2, 4, 6, 8, 10, 12],\n    \"n_initial_infections\": [1, 5, 10, 15, 20, 25]\n}\nThen we can run multiple mpdels for every combination of parameters using the batch_run() function.\nresults = mesa.batch_run(\n    model_cls=SIRModel,\n    parameters=params,\n    iterations=10,\n    max_steps=100,\n    number_processes=1,\n    data_collection_period=1,  # collect data at every step\n    display_progress=True,\n)\n\nresults_df = pd.DataFrame(results)\nresults_df.info()\nresults_df.head()\nLet’s create a small multiples plot to compare some high-level patterns across different parameter settings.\nmelted_df = pd.melt(\n    results_df,\n    # include 'RunId' to differentiate runs\n    id_vars=['n_initial_infections', 'infection_rate', 'Step', 'RunId'],\n    value_vars=['Susceptible', 'Infected', 'Recovered'],\n    var_name='State',\n    value_name='Proportion'\n)\n\n# create the FacetGrid\ng = sns.FacetGrid(\n    melted_df, col=\"n_initial_infections\", hue=\"State\",\n    col_wrap=3, height=4, aspect=1.5\n)\n\n# map the scatterplot for individual observations\ng.map_dataframe(\n    sns.scatterplot, x='Step', y='Proportion', alpha=0.005, edgecolor=None\n)\n\n# map the lineplot for aggregated data\ng.map_dataframe(\n    sns.lineplot, x='Step', y='Proportion', linewidth=2,\n)\n\ng.add_legend()\ng.set_axis_labels(\"Time Step\", \"Proportion\")\ng.set_titles(\"Initial Infections: {col_name}\")\n\nplt.savefig(\"images/sir_subplots.png\", dpi=300)\n\n\n\nBatch results for Model 2 with mean trends emphasized.\n\n\nNext, we aggreate the results for the sensitivity analysis.\naggregated_results = results_df.groupby([\"RunId\", \"infection_rate\", \"recovery_time_range\", \"max_agent_step_size\", \"n_initial_infections\"]).agg(\n    peak_infected=(\"Infected\", \"max\")\n).reset_index()\n\naggregated_results.head()\nLet’s plot the sensitivity analysis results. We’ll need to convert the recovery_time_range tuples into strings first.\naggregated_results['recovery_time_range_str'] = aggregated_results['recovery_time_range'].apply(lambda x: f\"{x[0]}-{x[1]}\")\nAnd then we can set up the plot.\nfig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey=True)\n\n# infection rate\nsns.boxplot(ax=axes[0, 0], x=\"infection_rate\", y=\"peak_infected\", data=aggregated_results)\naxes[0, 0].set_title(\"\\n(a) Sensitivity to Infection Rate\", loc=\"left\")\naxes[0, 0].set_xlabel(\"\\nInfection Rate\")\naxes[0, 0].set_ylabel(\"Peak Infected\\n\")\n\n# recovery time range (use the string version we just created)\nsns.boxplot(ax=axes[0, 1], x=\"recovery_time_range_str\", y=\"peak_infected\", data=aggregated_results)\naxes[0, 1].set_title(\"\\n(b) Sensitivity to Recovery Time Range\", loc=\"left\")\naxes[0, 1].set_xlabel(\"\\nRecovery Time Range\")\naxes[0, 1].set_ylabel(\"Peak Infected\\n\")\n\n# max agent step size\nsns.boxplot(ax=axes[1, 0], x=\"max_agent_step_size\", y=\"peak_infected\", data=aggregated_results)\naxes[1, 0].set_title(\"\\n(c) Sensitivity to Max Agent Step Size\", loc=\"left\")\naxes[1, 0].set_xlabel(\"\\nMax Agent Step Size\")\naxes[1, 0].set_ylabel(\"Peak Infected\\n\")\n\n# initial infections\nsns.boxplot(ax=axes[1, 1], x=\"n_initial_infections\", y=\"peak_infected\", data=aggregated_results)\naxes[1, 1].set_title(\"\\n(d) Sensitivity to Initial Infections\", loc=\"left\")\naxes[1, 1].set_xlabel(\"\\nInitial Infections\")\naxes[1, 1].set_ylabel(\"Peak Infected\\n\")\n\nplt.tight_layout()\nplt.savefig(\"images/sir_sensitivity_analysis.png\", dpi=300)\n\n\n\nSensitivity plot for our SIR model parameters and peak proportion infected across all model runs.\n\n\n\n\nAs an additional step, we can correlate our parameters with the peak proportion infected to get a sense of the relative influence of each parameter. To do so, we have to transform the recovery time ranges into a single number. There are a few ways we could do this, but to keep things simple we’ll take the mean of the two values.\naggregated_results[\"recovery_time_range_numeric\"] = aggregated_results[\"recovery_time_range\"].apply(lambda x: sum(x) / len(x))\nOnce we’ve done that, we’ll drop the original tuple (recovery_time_range) and it’s string representation (recovery_time_range_str). I’ll create a new dataframe for these modifications, since we’ll use the dropped variables in our second sensitivity analysis.\naggregated_results_4corr = aggregated_results.copy()\naggregated_results_4corr.drop(['recovery_time_range', 'recovery_time_range_str'], axis=1, inplace=True)\nAnd now we can correlate!\ncorrelation_matrix = aggregated_results_4corr.corr()\npeak_infected_correlations = correlation_matrix[\"peak_infected\"].drop(\n    [\"peak_infected\", \"RunId\"]\n)\nLet’s visualize the correlations using a line comparison plot.\nfor_plotting = {}\nfor parameter, correlation in zip(peak_infected_correlations.index, peak_infected_correlations.values):\n    for_plotting[parameter] = round(correlation, 4)\n\nplot_line_comparison(\n    for_plotting,\n    xrange=(0,1), # all positive correlations\n    print_decimals=True,\n    title=\"\",\n    xlabel='\\nCorrelations between model parameters and peak infection.',\n    filename=\"images/sir_correlations_params_peak_infection.png\"\n)\n\n\n\n\n\n\nFigure 1: Correlations between model parameters and peak infection proportion.\n\n\n\n\n\n\n\nNext, let’s analyze how different parameters affect the time it takes for the infection to reach its peak. \ntime_to_peak = results_df.loc[results_df.groupby(\"RunId\")[\"Infected\"].idxmax()]\ntime_to_peak = time_to_peak[[\"RunId\", \"Step\", \"Infected\"]].rename(\n    columns={\"Step\": \"time_to_peak\", \"Infected\": \"peak_infected\"}\n)\n\ntime_to_peak_merged = pd.merge(time_to_peak, aggregated_results, on=\"RunId\")\nLet’s visualize how the time to peak infection varies with different parameters.\nfig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey=True)\n\n# Infection rate\nsns.boxplot(ax=axes[0, 0], x=\"infection_rate\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[0, 0].set_title(\"\\n(a) Time to Peak Infection vs. Infection Rate\", loc=\"left\")\naxes[0, 0].set_xlabel(\"\\nInfection Rate\")\naxes[0, 0].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Recovery time range\nsns.boxplot(ax=axes[0, 1], x=\"recovery_time_range_str\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[0, 1].set_title(\"\\n(b) Time to Peak Infection vs. Recovery Time Range\", loc=\"left\")\naxes[0, 1].set_xlabel(\"\\nRecovery Time Range\")\naxes[0, 1].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Max agent step size\nsns.boxplot(ax=axes[1, 0], x=\"max_agent_step_size\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[1, 0].set_title(\"\\n(c) Time to Peak Infection vs. Max Agent Step Size\", loc=\"left\")\naxes[1, 0].set_xlabel(\"\\nMax Agent Step Size\")\naxes[1, 0].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Initial infections\nsns.boxplot(ax=axes[1, 1], x=\"n_initial_infections\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[1, 1].set_title(\"\\n(d) Time to Peak Infection vs. Initial Infections\", loc=\"left\")\naxes[1, 1].set_xlabel(\"\\nInitial Infections\")\naxes[1, 1].set_ylabel(\"Time to Peak Infection\\n\")\n\nplt.tight_layout()\nplt.savefig(\"images/sir_time_to_peak_analysis.png\", dpi=300)\n\n\n\n\n\n\nFigure 2: Sensitivity plot for our SIR model parameters and time to peak proportion infected across all model runs.\n\n\n\nThere is much more we could do here, but let’s move on to a different example."
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#learning-objectives",
    "href": "posts/2024-GESIS-5-1-abms.html#learning-objectives",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "By the end of this workshop, you will be able to:\n\nUnderstand the basic concepts and components of agent-based modeling.\nImplement a simple SIR model using Mesa, a Python library for agent-based modeling.\n\nIn this notebook, this is done via the course package icsspy, but the models we load here are the same ones I developed in the lecture.\n\nAnalyze the behavior of the SIR model under different conditions and parameters.\nExplore a Bounded Confidence Model to understand opinion dynamics.\nConduct sensitivity analysis and parameter sweeps to systematically investigate model behavior."
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#setup",
    "href": "posts/2024-GESIS-5-1-abms.html#setup",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "Let’s start by importing the necessary packages and configuring our environment.\nimport mesa\nfrom mesa.batchrunner import batch_run\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport networkx as nx\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport icsspy\nicsspy.set_style()\nfrom icsspy.networks import plot_line_comparison"
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#load-and-configure-the-models",
    "href": "posts/2024-GESIS-5-1-abms.html#load-and-configure-the-models",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "We’ll begin by exploring a classic Susceptible-Infected-Recovered (SIR) model. This model is often used to simulate the spread of infectious diseases.\n\n\nFirst, let’s load the SIRModel class from the course package and define the parameters for our first model, which simulates a disease with a high infection rate.\nfrom icsspy.abms.sir import SIRModel\n\nmodel_1_params = {'N': 100,\n 'grid_height': 40,\n 'grid_width': 40,\n 'infection_rate': 0.3,\n 'max_agent_step_size': 1,\n 'n_initial_infections': 2,\n 'n_iterations': 150,\n 'recovery_time_range': [14, 24]\n}\n\nprint(\"Model 1 Params\\n\", model_1_params)\nHere, we have a population of 100 agents on a 40x40 grid. The infection rate is set to 0.3, meaning there’s a 30% chance that a susceptible agent will become infected when in contact with an infected agent. The agents can move to adjacent cells, and their recovery time is randomly chosen between 14 and 24 time steps.\nNow, let’s run the model and store the results.\nmodel_1 = SIRModel(\n    n_initial_infections = model_1_params['n_initial_infections'],\n    grid_width = model_1_params['grid_width'],\n    grid_height = model_1_params['grid_height'],\n    N = model_1_params['N'],\n    infection_rate = model_1_params['infection_rate'],\n    recovery_time_range = model_1_params['recovery_time_range'],\n    max_agent_step_size = model_1_params['max_agent_step_size'],\n)\n\nfor i in range(model_1_params['n_iterations']):\n    model_1.step()\n\nm1res = model_1.datacollector.get_model_vars_dataframe()\nm1res\n\n\n\nmodel_2_params = {'N': 100,\n 'grid_height': 40,\n 'grid_width': 40,\n 'infection_rate': 0.15,\n 'max_agent_step_size': 1,\n 'n_initial_infections': 2,\n 'n_iterations': 150,\n 'recovery_time_range': [14, 24]\n}\n\nprint(\"\\nModel 2 Params\\n\", model_2_params)\nWe have kept most of the parameters the same as in Model 1, except for the infection rate, which is now 0.15, to simulate a less contagious disease.\nNow, let’s run Model 2.\nmodel_2 = SIRModel(\n    n_initial_infections = model_2_params['n_initial_infections'],\n    grid_width = model_2_params['grid_width'],\n    grid_height = model_2_params['grid_height'],\n    N = model_2_params['N'],\n    infection_rate = model_2_params['infection_rate'],\n    recovery_time_range = model_2_params['recovery_time_range'],\n    max_agent_step_size = model_2_params['max_agent_step_size'],\n)\n\nfor i in range(model_2_params['n_iterations']):\n    model_2.step()\n\nm2res = model_2.datacollector.get_model_vars_dataframe()\nm2res\n\n\n\nLet’s compare the infection curves from both models to see how the different infection rates affect the spread of the disease.\nfig, ax = plt.subplots()\nax.plot(m1res['Infected'], label=r'High transmissibility, $\\beta$=0.3')\nax.plot(m2res['Infected'], label=r'Low transmissibility $\\beta$=0.15')\nplt.xlabel(\"\\nDiscrete Steps in Time\")\nplt.ylabel(\"Proportion Infected\\n\")\nplt.legend(loc='upper right', fontsize=10)\nplt.savefig('output/sir_compare_models_1-2.png')\n\n\n\nIn addition to tracking the number of infected agents, it’s also useful to analyze how agents interact with each other during the simulation.\ninteraction_graphs = {}\ninteraction_graph_summaries = {}\n\nmodels = [model_1, model_2]\nfor i, model in enumerate(models, start=1):\n    wel = []\n    for agent in model.schedule.agents:\n        for k, v in agent.interactions.items():\n            wel.append((int(agent.unique_id), k, v))\n\n    G = nx.Graph()\n    G.add_weighted_edges_from(wel)\n    interaction_graphs[f'M{i}'] = G\n\n    avg_degree = round(sum(dict(G.degree()).values()) / float(G.number_of_nodes()), 2)\n    interaction_graph_summaries[f'M{i}'] = (G.number_of_nodes(), G.number_of_edges(), avg_degree)\ninteraction_graph_summaries = pd.DataFrame(interaction_graph_summaries).T\ninteraction_graph_summaries.columns = ['No. Nodes', 'No. Edges', 'Avg. Degree']\ninteraction_graph_summaries\n# Initialize an empty graph\nG = interaction_graphs['M2']\nweights = [d['weight'] for u, v, d in G.edges(data=True)]\n\nplt.figure(figsize=(8, 6))\nsns.ecdfplot(weights)\nplt.xlabel('Interaction Weight')\nplt.ylabel('ECDF')\nplt.title('ECDF of Interaction Weights')\nplt.grid(True)\nplt.savefig('output/compare_agent_networks.png', dpi=300)"
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#model-analysis",
    "href": "posts/2024-GESIS-5-1-abms.html#model-analysis",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "Model analysis is critical for understanding the behavior of an ABM and ensuring that it accurately reflects the real-world system being modeled. In this section, we’ll cover various analysis techniques, including descriptive, exploratory, comparative, and predictive analyses.\nSensitivity Analysis: Peak Proportion Infected Sensitivity analysis helps determine how changes in model parameters affect the outcomes. Here, we’ll focus on the peak proportion of infected agents as our outcome of interest.\nFirst, let’s set up a parameter sweep for the sensitivity analysis.\n\nparams = {\n    \"N\": 100,\n    \"grid_height\": 40,\n    \"grid_width\": 40,\n    \"infection_rate\": [0.2, 0.4, 0.6, 0.8,],\n    \"recovery_time_range\": [(7,14), (14,24)],\n    \"max_agent_step_size\": [2, 4, 6, 8, 10, 12],\n    \"n_initial_infections\": [1, 5, 10, 15, 20, 25]\n}\nThen we can run multiple mpdels for every combination of parameters using the batch_run() function.\nresults = mesa.batch_run(\n    model_cls=SIRModel,\n    parameters=params,\n    iterations=10,\n    max_steps=100,\n    number_processes=1,\n    data_collection_period=1,  # collect data at every step\n    display_progress=True,\n)\n\nresults_df = pd.DataFrame(results)\nresults_df.info()\nresults_df.head()\nLet’s create a small multiples plot to compare some high-level patterns across different parameter settings.\nmelted_df = pd.melt(\n    results_df,\n    # include 'RunId' to differentiate runs\n    id_vars=['n_initial_infections', 'infection_rate', 'Step', 'RunId'],\n    value_vars=['Susceptible', 'Infected', 'Recovered'],\n    var_name='State',\n    value_name='Proportion'\n)\n\n# create the FacetGrid\ng = sns.FacetGrid(\n    melted_df, col=\"n_initial_infections\", hue=\"State\",\n    col_wrap=3, height=4, aspect=1.5\n)\n\n# map the scatterplot for individual observations\ng.map_dataframe(\n    sns.scatterplot, x='Step', y='Proportion', alpha=0.005, edgecolor=None\n)\n\n# map the lineplot for aggregated data\ng.map_dataframe(\n    sns.lineplot, x='Step', y='Proportion', linewidth=2,\n)\n\ng.add_legend()\ng.set_axis_labels(\"Time Step\", \"Proportion\")\ng.set_titles(\"Initial Infections: {col_name}\")\n\nplt.savefig(\"images/sir_subplots.png\", dpi=300)\n\n\n\nBatch results for Model 2 with mean trends emphasized.\n\n\nNext, we aggreate the results for the sensitivity analysis.\naggregated_results = results_df.groupby([\"RunId\", \"infection_rate\", \"recovery_time_range\", \"max_agent_step_size\", \"n_initial_infections\"]).agg(\n    peak_infected=(\"Infected\", \"max\")\n).reset_index()\n\naggregated_results.head()\nLet’s plot the sensitivity analysis results. We’ll need to convert the recovery_time_range tuples into strings first.\naggregated_results['recovery_time_range_str'] = aggregated_results['recovery_time_range'].apply(lambda x: f\"{x[0]}-{x[1]}\")\nAnd then we can set up the plot.\nfig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey=True)\n\n# infection rate\nsns.boxplot(ax=axes[0, 0], x=\"infection_rate\", y=\"peak_infected\", data=aggregated_results)\naxes[0, 0].set_title(\"\\n(a) Sensitivity to Infection Rate\", loc=\"left\")\naxes[0, 0].set_xlabel(\"\\nInfection Rate\")\naxes[0, 0].set_ylabel(\"Peak Infected\\n\")\n\n# recovery time range (use the string version we just created)\nsns.boxplot(ax=axes[0, 1], x=\"recovery_time_range_str\", y=\"peak_infected\", data=aggregated_results)\naxes[0, 1].set_title(\"\\n(b) Sensitivity to Recovery Time Range\", loc=\"left\")\naxes[0, 1].set_xlabel(\"\\nRecovery Time Range\")\naxes[0, 1].set_ylabel(\"Peak Infected\\n\")\n\n# max agent step size\nsns.boxplot(ax=axes[1, 0], x=\"max_agent_step_size\", y=\"peak_infected\", data=aggregated_results)\naxes[1, 0].set_title(\"\\n(c) Sensitivity to Max Agent Step Size\", loc=\"left\")\naxes[1, 0].set_xlabel(\"\\nMax Agent Step Size\")\naxes[1, 0].set_ylabel(\"Peak Infected\\n\")\n\n# initial infections\nsns.boxplot(ax=axes[1, 1], x=\"n_initial_infections\", y=\"peak_infected\", data=aggregated_results)\naxes[1, 1].set_title(\"\\n(d) Sensitivity to Initial Infections\", loc=\"left\")\naxes[1, 1].set_xlabel(\"\\nInitial Infections\")\naxes[1, 1].set_ylabel(\"Peak Infected\\n\")\n\nplt.tight_layout()\nplt.savefig(\"images/sir_sensitivity_analysis.png\", dpi=300)\n\n\n\nSensitivity plot for our SIR model parameters and peak proportion infected across all model runs.\n\n\n\n\nAs an additional step, we can correlate our parameters with the peak proportion infected to get a sense of the relative influence of each parameter. To do so, we have to transform the recovery time ranges into a single number. There are a few ways we could do this, but to keep things simple we’ll take the mean of the two values.\naggregated_results[\"recovery_time_range_numeric\"] = aggregated_results[\"recovery_time_range\"].apply(lambda x: sum(x) / len(x))\nOnce we’ve done that, we’ll drop the original tuple (recovery_time_range) and it’s string representation (recovery_time_range_str). I’ll create a new dataframe for these modifications, since we’ll use the dropped variables in our second sensitivity analysis.\naggregated_results_4corr = aggregated_results.copy()\naggregated_results_4corr.drop(['recovery_time_range', 'recovery_time_range_str'], axis=1, inplace=True)\nAnd now we can correlate!\ncorrelation_matrix = aggregated_results_4corr.corr()\npeak_infected_correlations = correlation_matrix[\"peak_infected\"].drop(\n    [\"peak_infected\", \"RunId\"]\n)\nLet’s visualize the correlations using a line comparison plot.\nfor_plotting = {}\nfor parameter, correlation in zip(peak_infected_correlations.index, peak_infected_correlations.values):\n    for_plotting[parameter] = round(correlation, 4)\n\nplot_line_comparison(\n    for_plotting,\n    xrange=(0,1), # all positive correlations\n    print_decimals=True,\n    title=\"\",\n    xlabel='\\nCorrelations between model parameters and peak infection.',\n    filename=\"images/sir_correlations_params_peak_infection.png\"\n)\n\n\n\n\n\n\nFigure 1: Correlations between model parameters and peak infection proportion."
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#sensitivity-analysis-time-to-peak-proportion-infected",
    "href": "posts/2024-GESIS-5-1-abms.html#sensitivity-analysis-time-to-peak-proportion-infected",
    "title": "Simulation and Agent-based Modelling",
    "section": "",
    "text": "Next, let’s analyze how different parameters affect the time it takes for the infection to reach its peak. \ntime_to_peak = results_df.loc[results_df.groupby(\"RunId\")[\"Infected\"].idxmax()]\ntime_to_peak = time_to_peak[[\"RunId\", \"Step\", \"Infected\"]].rename(\n    columns={\"Step\": \"time_to_peak\", \"Infected\": \"peak_infected\"}\n)\n\ntime_to_peak_merged = pd.merge(time_to_peak, aggregated_results, on=\"RunId\")\nLet’s visualize how the time to peak infection varies with different parameters.\nfig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey=True)\n\n# Infection rate\nsns.boxplot(ax=axes[0, 0], x=\"infection_rate\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[0, 0].set_title(\"\\n(a) Time to Peak Infection vs. Infection Rate\", loc=\"left\")\naxes[0, 0].set_xlabel(\"\\nInfection Rate\")\naxes[0, 0].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Recovery time range\nsns.boxplot(ax=axes[0, 1], x=\"recovery_time_range_str\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[0, 1].set_title(\"\\n(b) Time to Peak Infection vs. Recovery Time Range\", loc=\"left\")\naxes[0, 1].set_xlabel(\"\\nRecovery Time Range\")\naxes[0, 1].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Max agent step size\nsns.boxplot(ax=axes[1, 0], x=\"max_agent_step_size\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[1, 0].set_title(\"\\n(c) Time to Peak Infection vs. Max Agent Step Size\", loc=\"left\")\naxes[1, 0].set_xlabel(\"\\nMax Agent Step Size\")\naxes[1, 0].set_ylabel(\"Time to Peak Infection\\n\")\n\n# Initial infections\nsns.boxplot(ax=axes[1, 1], x=\"n_initial_infections\", y=\"time_to_peak\", data=time_to_peak_merged)\naxes[1, 1].set_title(\"\\n(d) Time to Peak Infection vs. Initial Infections\", loc=\"left\")\naxes[1, 1].set_xlabel(\"\\nInitial Infections\")\naxes[1, 1].set_ylabel(\"Time to Peak Infection\\n\")\n\nplt.tight_layout()\nplt.savefig(\"images/sir_time_to_peak_analysis.png\", dpi=300)\n\n\n\n\n\n\nFigure 2: Sensitivity plot for our SIR model parameters and time to peak proportion infected across all model runs.\n\n\n\nThere is much more we could do here, but let’s move on to a different example."
  },
  {
    "objectID": "posts/2024-GESIS-5-1-abms.html#bounded-confidence-models",
    "href": "posts/2024-GESIS-5-1-abms.html#bounded-confidence-models",
    "title": "Simulation and Agent-based Modelling",
    "section": "Bounded Confidence Models",
    "text": "Bounded Confidence Models\nfrom mesa import Agent\nfrom mesa import Model\nfrom mesa.space import MultiGrid\nfrom mesa.time import RandomActivation\nfrom mesa.datacollection import DataCollector\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport yaml\n\nimport icsspy\nicsspy.set_style()\nfrom icsspy.abms.bounded_confidence import BoundedConfidenceModel\n#| echo: false\nwith open('_variables.yml', 'r') as file:\n    params = yaml.safe_load(file)\n\nmodel_params = params.get('bounded_confidence_model_1')\npprint(model_params)\nmodel_params = {'N': 100,\n 'epsilon': 0.5,\n 'grid_height': 40,\n 'grid_width': 40,\n 'max_agent_step_size': 1,\n 'n_iterations': 1500\n}\npprint(model_params)\n\nRun the Model & Collect Data\nLet’s run the model with the parameters we’ve defined and collect the data.\nmodel = BoundedConfidenceModel(\n    grid_width=model_params['grid_width'],\n    grid_height=model_params['grid_height'],\n    N=model_params['N'],\n    epsilon=model_params['epsilon'],\n    max_agent_step_size=model_params['max_agent_step_size'],\n)\n\nfor i in range(model_params['n_iterations']):\n    model.step()\n\nresults = model.datacollector.get_agent_vars_dataframe().reset_index()\nresults.head(10)\n\n\nPlot Opinion Distributions\nNow, we’ll visualize the evolution of the opinion distribution over time.\ngrouped = results.groupby('Step')\n\nplt.figure(figsize=(10, 6))\n\nfor name, group in grouped:\n    sns.kdeplot(group['Opinion'], color='C0', alpha=0.2)\n\nplt.xlabel('\\nOpinion')\nplt.ylabel('Density\\n')\ntitle = \"Bounded Confidence Model\\n\" + r\"$\\epsilon$\" + f\" = {model_params['epsilon']}\\n\"\nplt.title(title, loc='left')\nplt.grid(True)\nplt.savefig('output/bounded-confidence-opinion-distribution-evolution.png', dpi=300)\n\n\n\n\n\n\nFigure 3: A collection of KDE plots showing the evolution of the opinion distribution at each time step in the model.\n\n\n\n\n\nPlot Opinions Over Time\nLet’s visualize how opinions change over time for individual agents.\ntime_steps = results['Step']\nopinions = results['Opinion']\ntitle = \"Bounded Confidence Model\\n\" + r\"$\\epsilon$\" + f\" = {model_params['epsilon']}\\n\"\n\nplt.figure(figsize=(12, 6))\nsc = plt.scatter(time_steps, opinions, c=opinions, cmap='coolwarm', alpha=0.5, s=10)\ncbar = plt.colorbar(sc)\ncbar.set_label('Opinion')\nplt.xlabel('Time (Steps)')\nplt.ylabel('Opinion')\nplt.title(title, loc='left')\nplt.xlim(time_steps.min(), time_steps.max())\nplt.ylim(-1.01, 1)\nplt.savefig('output/bounded_confidence_epsilon_one_run.png', dpi=300)\n\n\n\n\n\n\nFigure 4: Three opinion clusters forming over time in a single run of a BCM with \\(\\epsilon\\) = 5. Note the center and two extremes.\n\n\n\n\n\nExperimenting with \\(\\epsilon\\)\nFinally, let’s run the simulation with different \\(\\epsilon\\) values and compare the results.\n\ndef run_simulation_with_different_epsilons(model_params, epsilon_values):\n    all_results = []\n\n    for epsilon in epsilon_values:\n        model = BoundedConfidenceModel(\n            grid_width=model_params['grid_width'],\n            grid_height=model_params['grid_height'],\n            N=model_params['N'],\n            epsilon=epsilon,\n            max_agent_step_size=model_params['max_agent_step_size'],\n        )\n\n        for i in range(model_params['n_iterations']):\n            model.step()\n\n        epsilon_results = model.datacollector.get_agent_vars_dataframe().reset_index()\n        epsilon_results['Epsilon'] = epsilon  # Add epsilon to track different values\n        all_results.append(epsilon_results)\n\n    # Combine all results into a single DataFrame\n    combined_results = pd.concat(all_results, ignore_index=True)\n    return combined_results\nLet’s run the model for a range of \\(\\epsilon\\) values and then plot the results.\nepsilon_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nepsilon_results = run_simulation_with_different_epsilons(\n    model_params, epsilon_values\n)\n\nepsilon_results.sample(10)\nLet’s create some subplots to compare runs for each of our \\(\\epsilon\\) values.\ngrouped = epsilon_results.groupby('Epsilon')\n\nfig, axs = plt.subplots(\n    11, 1,\n    figsize=(12, 30),\n    sharex=True, sharey=True,\n    constrained_layout=True\n)\n\nfor (epsilon, group), ax in zip(grouped, axs):\n    time_steps = group['Step']\n    opinions = group['Opinion']\n\n    sc = ax.scatter(\n        time_steps,\n        opinions,\n        c=opinions,\n        cmap='coolwarm',\n        alpha=0.5,\n        s=10\n    )\n\n    cbar = fig.colorbar(sc, ax=ax)\n    # cbar.set_label('Opinion')\n\n    ax.set_title(r\"$\\epsilon$\" + f\" = {epsilon}\", loc='left')\n\n    ax.set_xlim(time_steps.min(), time_steps.max())\n    ax.set_ylim(-1.01, 1)\n\n# custom positions for shared x and y labels\nfig.text(\n    0.5, -0.01,\n    'Time (Steps)',\n    ha='center',\n    va='center',\n    fontsize=18\n)\n\nfig.text(\n    -0.01, 0.5,\n    'Opinion',\n    ha='center',\n    va='center',\n    rotation='vertical',\n    fontsize=18\n)\n\nplt.savefig('output/epsilon_comparison_one_run.png', dpi=300)\n\n\n\n\n\n\nFigure 5: Results of a parameter sweep on \\(\\epsilon\\). Note tendencies towards fragmentation, polarization, and consensus at different \\(\\epsilon\\) values."
  },
  {
    "objectID": "posts/2024-GESIS-4-1-network-analysis-political-blogs.html",
    "href": "posts/2024-GESIS-4-1-network-analysis-political-blogs.html",
    "title": "Network Analysis with Graph-Tool (Political Blogs Network)",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/2024-GESIS-1-setup.html",
    "href": "posts/2024-GESIS-1-setup.html",
    "title": "Setup",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/2024-GESIS-4-2-network-analysis-enron.html",
    "href": "posts/2024-GESIS-4-2-network-analysis-enron.html",
    "title": "Network Analysis with Graph-Tool (Enron Email Network)",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html",
    "title": "Working with the YouTube API",
    "section": "",
    "text": "This tutorial will guide you through working with the YouTube API to collect data from YouTube channels, including video metadata and comments. You’ll gain hands-on experience with the API and learn how to efficiently manage API calls, especially when dealing with rate limits.\nThe YouTube API is well-documented, but it can be a challenging API to start with. I’ve developed a Python module as part of the course package icsspy to make things a little easier. If you are comfortable with the content I present here and want to learn more, I would encourage you to review the code I wrote for the package, which you can find here: link.\n\n\nIn this tutorial, you will learn how to:\n\nObtain and securely store a YouTube API key\nUse the YouTube API to collect data about channels and videos\nProcess and analyze data collected from the YouTube API\nHandle API rate limits and errors effectively"
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#learning-objectives",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#learning-objectives",
    "title": "Working with the YouTube API",
    "section": "",
    "text": "In this tutorial, you will learn how to:\n\nObtain and securely store a YouTube API key\nUse the YouTube API to collect data about channels and videos\nProcess and analyze data collected from the YouTube API\nHandle API rate limits and errors effectively"
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#get-a-youtube-api-key",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#get-a-youtube-api-key",
    "title": "Working with the YouTube API",
    "section": "Get a YouTube API Key",
    "text": "Get a YouTube API Key\nThe first thing you need to do is get yourself an API key. You can do that by following these steps, each described in more detail below.\n\nLog into / sign up for a Google Account\nGo to the Google Cloud Console website\nCreate a new project\nEnable the YouTube Data API v3\nCreate an API key for the YouTube Data API v3\nStore Your API Key Securely\nRestrict your API key\n\n(1) First, you’ll need to sign up for a Google account if you don’t already have one. (2) Next, open the Google Cloud Console webpage. You should see something like this:\n\n(3) Use the dropdown menu to the right of Google Cloud to create a New Project. If you already have a project setup, it may show the name of the current project. In my case, it shows INTRO-CSS. Give your project an informative name and leave the location field as is. Press Create.\n\n(4) Next, you’ll need to enable the YouTube Data API v3. Under “Quick Access”, click on APIs & Services and select Library. Type YouTube into the search bar and then select YouTube Data API v3. A new page will load with “1 result”. Click the button and then enable the YouTube API on the new page.\n\n(5) Now you can create an API key for the YouTube Data API v3. Select Credentials from the left-side navigation pane. When the page loads, click the Create Credentials button and select API Key.\n\nYou should see a popup that looks something like this:\n\n(6) Your API key is like a password; you should treat it as such. Copy your key, store it someplace secure,1, and then close the popup. You should see your new key listed under API Keys with an orange alert icon to the right of your key name. In my case, the newly created key is API key 2.\n1 I recommend using Bitwarden or another password manager.\n(7) Finally, you’ll want to restrict your API Key. Click the three dots under Actions to Edit API Key. You should see something like this:\n\nUnder API Restrictions, select Restrict key and then select YouTube Data API v3 from the drop-down menu. Click Save.\n\nYour API key is now ready to use!"
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#using-your-api-key-securely",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#using-your-api-key-securely",
    "title": "Working with the YouTube API",
    "section": "Using Your API Key Securely",
    "text": "Using Your API Key Securely\nThere are a few ways you can securely use your API key. The approach we’ll use here is to store your key(s) in a special file that we can load whenever we need to authenticate with YouTube. From the command line, create a new file called .env in the project directory.\ncd computational-social-science\ntouch .env\nYou can edit this file using your preferred text editor. Each line of the file should contain the name and the value of your API key (so just 1 line if you are using 1 API key). The name itself doesn’t matter, but it’s useful to give it a name that corresponds to the name of the project you created to get the API key. The example below is a randomly-generated fake API key assigned to the name GESIS.\nGESIS='GEzaLyB69Xh5yz3QRsdP-X8QeLMpgWuva-XmWKh'\nIf you have more than one API key (which can come in handy), make sure each key is on its own line.2\n2 If your code is under version control, make sure you don’t commit the .env file to any public repositories. If you’re using git, add .env to your .gitignore file."
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#setup",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#setup",
    "title": "Working with the YouTube API",
    "section": "Setup",
    "text": "Setup\nNow that you have your API keys setup, load the packages we’ll use in this tutorial. Most of the heavy listing will be done by the YouTube module of the course package, icsspy.\nimport pandas as pd\nimport yaml\n\nimport icsspy\nimport icsspy.utils as utils\nimport icsspy.youtube as yt\nimport icsspy.cleaners as clean\nWe can load our API key(s) using a utility function from icsspy and then initialize the YouTubeAPI class I developed to simplify using multiple API keys at once,3 which is useful if you need to switch between them due to rate limits.\n3 Follow the instructions above to make sure you have your .env file setup correctly.with open(\"input/config.yaml\", \"r\") as file:\n    config = yaml.safe_load(file)\n\nKEY_NAMES = config.get(\"list_youtube_api_key_names\", \"YOUTUBE_API_KEY\")\nAPI_KEYS = utils.load_api_key_list(KEY_NAMES)\nYOUTUBE_API = yt.YouTubeAPI(API_KEYS)\n\nUnderstanding the YouTubeAPI Class\nThe YouTubeAPI class is designed to handle interactions with the YouTube API, specifically error handling and the task of using multiple API keys at once and an exponential back-off strategy to avoid rate limiting and handle errors automatically.\nAs you can see in the code block above, we initialize the YouTubeAPI class with a list of API keys. If you use the load_api_key_list() function and a config file, this is a simple process. Once you initialize it, the class:\n\nCreates a “service” object, which is the main interface to the YouTube API and allows us to send requests and receive responses. It does this using the googleapiclient.discovery.build() function and the build_service() method.\nAutomatically switches API keys if one API key hits a rate limit.\nExecutes requests using an execute_request() method. This method handles the actual sending of requests to the API and includes error handling to manage rate limits and retries. It also uses an exponential backoff strategy to avoid overwhelming the API with too many requests in a short time."
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#collect-data-from-the-talks-at-google-channel",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#collect-data-from-the-talks-at-google-channel",
    "title": "Working with the YouTube API",
    "section": "Collect Data from the Talks at Google Channel",
    "text": "Collect Data from the Talks at Google Channel\nIn this example, we’re going to collect data from the talksatgoogle YouTube channel.\n\nGet the YouTube Channel ID\nTo collect data from a YouTube channel, we need to get its YouTube channel ID. We can do this using the get_channel_id() function from the icsspy course package. This function is robust in handling different ways users might identify YouTube channels, making it easier to work with the API.\nget_channel_id() tries two methods to find the channel’s ID:\n\nCustom URL Search: It first checks if the provided channel argument is a custom URL. Many YouTube channels use custom URLs for easier access.\nUsername Search: If the custom URL search fails, it then tries to get the channel ID using the YouTube username.\n\nIf both methods fail, the function returns None.\nchannel = 'talksatgoogle'\n\nchannel_id = yt.get_channel_id(YOUTUBE_API, channel)\nprint(f'The YouTube Channel ID for {channel} is {channel_id}.')"
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#use-the-channel-id-to-collect-video-data",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#use-the-channel-id-to-collect-video-data",
    "title": "Working with the YouTube API",
    "section": "Use the Channel ID to Collect Video Data",
    "text": "Use the Channel ID to Collect Video Data\nWith the channel ID in hand, we can retrieve a list of video IDs associated with the channel using the get_channel_video_ids() function, which sends a request to the YouTube API to get the channel’s uploads playlist ID and then iteratively fetches all the channel’s public video IDs.\nWe can pass the resulting lists of video IDs to the get_channel_video_data() function, which makes another API query to collect data such as the video’s title, description, statistics (like views and likes), and other metadata.\nvideo_ids = yt.get_channel_video_ids(YOUTUBE_API, channel_id)\nvideo_details = yt.get_channel_video_data(YOUTUBE_API, video_ids)\nprint(f\"Collected data on {len(video_details)} videos from {channel}.\")\n\nutils.save_json(video_details, 'output/talks_at_google_videos.json')\nLike most modern APIs, the YouTube API returns data in JSON format. We’ll store this data by writing the JSON to disk, which will allow us to easily reload the data later without needing to re-query the YouTube API. For example, we can load the JSON data – from disk or memory – directly into a Pandas dataframe.\nvideos = pd.json_normalize(video_details)\nvideos.to_csv('output/videos.csv', index=False)\n\nvideos.info()\nNow that we have data on python len(video_details) videos, let’s query the YouTube API to collect data on the comments on these videos."
  },
  {
    "objectID": "posts/2024-GESIS-2-2-obtaining-data-apis.html#process-channel-data-and-prepare-to-collect-video-comments",
    "href": "posts/2024-GESIS-2-2-obtaining-data-apis.html#process-channel-data-and-prepare-to-collect-video-comments",
    "title": "Working with the YouTube API",
    "section": "Process Channel Data and Prepare to Collect Video Comments",
    "text": "Process Channel Data and Prepare to Collect Video Comments\nYou’ll likely end up running the code in this notebook several times (or more). Each time you’ll query the YouTube API, potentially re-collecting data you’ve already collected. Since collecting comments can involve a very large number of API calls, and API calls are expensive in terms of quota, we’ll do some prep work to minimize our API calls and the risk of hitting the rate limit.\nvideos[\"statistics.commentCount\"] = pd.to_numeric(\n  videos[\"statistics.commentCount\"], errors='coerce'\n)\n\nprobably_no_public_comments = videos[videos[\"statistics.commentCount\"].isna()][\"id\"].tolist()\nno_public_comments = videos[videos[\"statistics.commentCount\"] == 0][\"id\"].tolist()\nhas_public_comments = videos[videos[\"statistics.commentCount\"] &gt; 0][\"id\"].tolist()\nWe’ll check for the file output/talks_at_google_video_comments.csv, which is created a little later in this tutorial; if it already exists, we’ll get the IDs for videos we’ve already downloaded and skip their collection. If the file hasn’t been created yet (i.e., this is the first time you’re running this code), then it will collect everything.\nno_redownloading = True\n\nif no_redownloading is True:\n    try:\n        already_downloaded = pd.read_csv(\"output/comments.csv\")\n        already_downloaded = already_downloaded[\"video_id\"].unique().tolist()\n        has_public_comments = [\n            video\n            for video in has_public_comments\n            if video not in set(already_downloaded)\n        ]\n    except (FileNotFoundError, pd.errors.EmptyDataError):\n        already_downloaded = []\nWe’ll also skip the videos in “probably_no_public_comments,” but if you want to try collecting them, just uncomment the second line below.\ncollect = has_public_comments\n# collect = has_public_comments + probably_no_public_comments\n\noverwrite = False # set this to true the first time you run it; then false\nWith this prep work done, we can collect comment data using the collect_comments_for_videos() function, which iterates over a list of video IDs and collects comments for each video. It starts by opening a CSV file to store the comments. If overwrite is True (see the code block above), it will create a new file; otherwise, it appends to an existing file. For each video, it calls the get_video_comments() function, which fetches the comments using the YouTube API. The comments are then written to the CSV file in real-time.\ncollect_comments_for_videos() includes error handling for cases where comments might be disabled or where rate limits are exceeded, which means the function can handle issues that come up without crashing. This makes it useful for collecting large amounts of data.\nThere are a lot of comments to collect, so this code will take a while to run. There’s a progress bar to let you know what to expect. Once it’s up and running, you’ll want to leave it for a bit and come back.\nall_comments = yt.collect_comments_for_videos(\n    YOUTUBE_API, collect, \"output/comments.csv\", overwrite=overwrite\n)\nFinally, after collecting the comments, we can load them into a dataframe and take a look.\nall_comments = pd.read_csv('output/comments.csv')\nall_comments.info()\nall_comments.head()"
  },
  {
    "objectID": "posts/Just Another Technical Blog.html",
    "href": "posts/Just Another Technical Blog.html",
    "title": "Hello World!",
    "section": "",
    "text": "I’ve been meaning to start doing some technical blogging for ~13 years, but I’ve always been too busy. Now that I have newborn twins, I figure it’s a good time to start. 🤣"
  },
  {
    "objectID": "posts/2024-GESIS-6-1-project.html",
    "href": "posts/2024-GESIS-6-1-project.html",
    "title": "Project Work",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/2024-GESIS-3-2-text-analysis-transformers.html",
    "href": "posts/2024-GESIS-3-2-text-analysis-transformers.html",
    "title": "Text Analysis with Transformers",
    "section": "",
    "text": "👋 Hi there!\nIf you’re looking for the course materials for my GESIS Fall Seminar course, you’re in the right place! I’m finalizing things before the course launches. This content will be available no later than Thursday August 29th. See you soon!\nJohn\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "publications/chapters/SHSNA-Introduction.html",
    "href": "publications/chapters/SHSNA-Introduction.html",
    "title": "Intoduction",
    "section": "",
    "text": "PREPRINT VERSION\nThis new edition of the Sage Handbook of Social Network Analysis aims to build on the success of its predecessor, bringing together in one place a comprehensive overview of social network analysis produced by leading international scholars in the field. We present authoritative accounts of its history, theories, and methods, and a wide-ranging review of the various disciplines and topics to which it has made important contributions. Most of the topics covered in the first edition are included here, but in thoroughly revised or new chapters, many by new authors. We have also included chapters on a number of new topics that have experienced significant development since the first edition appeared, in order to ensure that the volume is completely up to date and so continues to meet the needs and expectations of our readers. In what follows, we provide a high-level overview of the development and growth of social network analysis and discuss several ideas that are foundational to the field."
  },
  {
    "objectID": "publications/chapters/SHSNA-Introduction.html#footnotes",
    "href": "publications/chapters/SHSNA-Introduction.html#footnotes",
    "title": "Intoduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOverviews of the history of social network analysis can be found in Freeman (2004); Prell (2012), and Scott (2017: Chapter 2).↩︎\nThe others consist of function words, as well as collections of words that are so common due to (a) their prevalence in nearly all SNA topics, or (b) article writing conventions.↩︎\nOf course, like all methods, this approach is imperfect. Our two-step approach is designed to avoid false positives due to the widespread use of “network” as a metaphor, to differentiate between social network analysis and actor network theory, and so on. While this method may miss some social network analyses, we think the probability of a publication using social network analysis citing at least one article in Social Networks is very high. Among these articles, we consider any that contain ‘network’ to be an SNA publication. The final step in our approach mirrors that of others (CITES).↩︎\nMatrices are also used for other types of network data, such as bipartite, multi-level, and multi-plex networks.↩︎"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-i-foundations",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-i-foundations",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part I: Foundations",
    "text": "Part I: Foundations\nChapter 1 | Setting up your open source scientific computing environment\nChapter 2 | Python programming: The basics\nChapter 3 | Python programming: Data structures, functions and files\nChapter 4 | Collecting data from Application Programming Interfaces (APIs)\nChapter 5 | Collecting data from the web (Scraping)\nChapter 6 | Processing structured data\nChapter 7 | Visualisation and exploratory data analysis\nChapter 8 | Latent factors and components"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-ii-fundamentals-of-text-analysis",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-ii-fundamentals-of-text-analysis",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part II: Fundamentals of text analysis",
    "text": "Part II: Fundamentals of text analysis\nChapter 9 | Processing natural language data\nChapter 10 | Iterative text analysis\nChapter 11 | Exploratory text analysis\nChapter 12 | Text similarity and latent semantic space"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-iii-fundamentals-of-network-analysis",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-iii-fundamentals-of-network-analysis",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part III: Fundamentals of network analysis",
    "text": "Part III: Fundamentals of network analysis\nChapter 13 | Social networks and relational thinking\nChapter 14 | Connection and clustering in social networks\nChapter 15 | Influence, inequality and power in social networks\nChapter 16 | Going viral: Modelling the epidemic spread of simple contagions\nChapter 17 | Not so fast: Modelling the diffusion of complex contagions"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-iv-research-ethics-and-machine-learning",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-iv-research-ethics-and-machine-learning",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part IV: Research ethics and machine learning",
    "text": "Part IV: Research ethics and machine learning\nChapter 18 | Research ethics, politics and practices\nChapter 19 | Machine learning: Symbolic and connectionist\nChapter 20 | Supervised learning with regression and cross-validation\nChapter 21 | Supervised learning with tree-based models\nChapter 22 | Neural networks and deep learning\nChapter 23 | Developing neural network models with Keras and Tensorflow"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-v-bayesian-machine-learning-and-probabilistic-programming",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-v-bayesian-machine-learning-and-probabilistic-programming",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part V: Bayesian machine learning and probabilistic programming",
    "text": "Part V: Bayesian machine learning and probabilistic programming\nChapter 24 | Statistical machine learning and generative models\nChapter 25 | Probability: A primer\nChapter 26 | Approximate posterior inference with stochastic sampling and MCMC"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-vi-bayesian-data-analysis-and-latent-variable-modelling-with-relational-and-text-data",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-vi-bayesian-data-analysis-and-latent-variable-modelling-with-relational-and-text-data",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part VI: Bayesian data analysis and latent variable modelling with relational and text data",
    "text": "Part VI: Bayesian data analysis and latent variable modelling with relational and text data\nChapter 27 | Bayesian regression models with probabilistic programming\nChapter 28 | Bayesian hierarchical regression modelling\nChapter 29 | Variational Bayes and the craft of generative topic modelling\nChapter 30 | Generative network analysis with Bayesian stochastic blockmodels"
  },
  {
    "objectID": "publications/books/Doing-Computational-Social-Science.html#part-vii-embeddings-transformer-models-and-named-entity-recognition",
    "href": "publications/books/Doing-Computational-Social-Science.html#part-vii-embeddings-transformer-models-and-named-entity-recognition",
    "title": "Doing Computational Social Science: A Practical Introduction",
    "section": "Part VII: Embeddings, transformer models and named entity recognition",
    "text": "Part VII: Embeddings, transformer models and named entity recognition\nChapter 31 | Can we model meaning?: Contextual representation and neural word embeddings\nChapter 32 | Named entity recognition, transfer learning and transformer models"
  },
  {
    "objectID": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-a-general-issues",
    "href": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-a-general-issues",
    "title": "The SAGE Handbook of Social Network Analysis",
    "section": "PART A: GENERAL ISSUES",
    "text": "PART A: GENERAL ISSUES\nChapter 2 | Introducing Social Network AnalysisChristina Prell and David R. Schaefer\nChapter 3 | Social Networks and Social CategoriesRonald Breiger and Robin Wagner-Pacifici\nChapter 4 | Social Networks and Computational Social ScienceJames Kitts, Helene Grogan, and Kevin Lewis\nChapter 5 | Social Networks and Relational SociologyAnn Mische and Jan Fuhse"
  },
  {
    "objectID": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-b-applications",
    "href": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-b-applications",
    "title": "The SAGE Handbook of Social Network Analysis",
    "section": "PART B: APPLICATIONS",
    "text": "PART B: APPLICATIONS\nChapter 6 | Social-ecological NetworksÖrjan Bodin\nChapter 7 | The Evolution of Environmental Policy Network AnalysisTyler A. Scott, Mark Lubell, and Gwen Arnold\nChapter 8 | Health Behaviours and OutcomesKayla de la Haye\nChapter 9 | Political and Policy NetworksMario Diani\nChapter 10 | Social Movements and Collective ActionDavid Tindall\nChapter 11 | Gender and Social NetworksElisa Bellotti\nChapter 12 | Why can’t we be friends? Understanding ethnic relations through network analysisRochelle Cote\nChapter 13 | Culture and NetworksOmar Lizardo\nChapter 14 | Semantic and Cultural NetworksSarah Shugars and Sandra González-Bailón\nChapter 15 | Cognition and Social NetworksMatthew E. Brashears and Victoria Money\nChapter 16 | Scientific NetworksDonghyun Kang and James Evans\nChapter 17 | Crime and NetworksMarie Ouellet and Logan Ledford\nChapter 18 | Historical Network Analysis: Two Problems of ScaleIan Kumekawa\nChapter 19 | The Paradoxes of Behavior Change and the New Science of Network DiffusionDamon Centola\nChapter 20 | Network Interventions: Using Social Networks to Accelerate Diffusion of InnovationsThomas W. Valente\nChapter 21 | Social Media and Digital NetworksAnabel Quan-Haase, Lyndsay Foisey, and Riley McLaughlin\nChapter 22 | Social CapitalBeate Völker\nChapter 23 | Social SupportLijun Song and Zhe Zhang\nChapter 24 | Corporate NetworksWilliam K. Carroll, Jouke Huijzer and J. P. Sapinski"
  },
  {
    "objectID": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-c-concepts-methods",
    "href": "publications/books/Sage-Handbook-Social-Network-Analysis.html#part-c-concepts-methods",
    "title": "The SAGE Handbook of Social Network Analysis",
    "section": "PART C: CONCEPTS & METHODS",
    "text": "PART C: CONCEPTS & METHODS\nChapter 25 | International Trade NetworksChristina Prell, James Hollway, Petr Matous, Yasuyuki Todo\nChapter 26 | CentralityMartin Everett and Steve Borgatti\nChapter 2 | Structural Cohesion and Cohesive GroupsJames Moody and Peter J. Mucha\nChapter 28 | Multimodal Social Network AnalysisLorien Jasny\nChapter 29 | Blockmodeling, Positions and RolesPatrick Doreian, Anuška Ferligoj, and Vladimir Batagelj\nChapter 30 | Inferential Network Clustering with Hierarchical Bayesian Stochastic BlockmodelsPierson Browne, Tyler Crick, and John McLevey\nChapter 31 | Personal Networks and Egocentric AnalysisBrea Perry, Adam Roth, and Mario Small\nChapter 32 | Multilevel Network AnalysisEmmanuel Lazega & Peng Wang\nChapter 33 | Exponential Random Graph ModelsJohan Koskinen\nChapter 4 | Network DynamicsTom A.B. Snijders and Christian E.G. Steglich\nChapter 3 | Relational Event ModelsAaron Schecter and Noshir Contractor\nChapter 36 | Latent Position Network ModelsHardeep Kaur, Riccardo Rastelli, Nial Friel, and Adrian E. Raftery\nChapter 37 | Negative Ties and Signed NetworksFilip Agneessens\nChapter 38 | Qualitative and Mixed MethodsBetina Hollstein\nChapter 39 | Spatial Analysis of Social NetworksJohn R. Hipp\nChapter 40 | Social Network Data Collection: Principles and Modalitiesjimi adams and Miranda Lubbers\nChapter 41 | Missing Network DataRobert W. Krause and Mark Huisman\nChapter 42 | Scientific Software for Network AnalysisPierson Browne, Adam Howe, Yasmin Koop-Monteiro, Yixi Yang, and John McLevey"
  },
  {
    "objectID": "pages/research.html",
    "href": "pages/research.html",
    "title": "Research",
    "section": "",
    "text": "I am currently…"
  },
  {
    "objectID": "pages/research.html#polarization-and-public-opinion-dynamics",
    "href": "pages/research.html#polarization-and-public-opinion-dynamics",
    "title": "Research",
    "section": "Polarization and Public Opinion Dynamics",
    "text": "Polarization and Public Opinion Dynamics"
  },
  {
    "objectID": "pages/research.html#probabilistic-stance-models",
    "href": "pages/research.html#probabilistic-stance-models",
    "title": "Research",
    "section": "Probabilistic Stance Models",
    "text": "Probabilistic Stance Models"
  },
  {
    "objectID": "pages/courses.html",
    "href": "pages/courses.html",
    "title": "Courses",
    "section": "",
    "text": "ID\nCourse\nLevel\nDelivery\nLink\n\n\n\n\nFCIT 607\nData, Methods, and Models for Future Cities\nGraduate\nRemote\nWebsite\n\n\nINTEG 640\nComputational Social Science\nGraduate\nOnline\nWebsite\n\n\nINTEG 440\nComputational Social Science\nUndergraduate\nOnline\nWebsite\n\n\nINTEG 340\nResearch Methods\nUndergraduate\nIn person\nWebsite\n\n\nINTEG 240\nBullshit, Bias, & Bad Arguments\nUndergraduate\nRemote\nWebsite\n\n\nINTEG 140\nThe Art & Science of Learning\nUndergraduate\nIn person\nWebsite"
  },
  {
    "objectID": "pages/lab.html",
    "href": "pages/lab.html",
    "title": "NETLAB",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "pages/lab.html#student-members",
    "href": "pages/lab.html#student-members",
    "title": "NETLAB",
    "section": "Student Members",
    "text": "Student Members\n\nTyler Crick (PhD)\nKarmvir Padda (PhD)\nSasha Graham (PhD)\nRomina Heshimi (UG)"
  },
  {
    "objectID": "pages/lab.html#lab-alumni",
    "href": "pages/lab.html#lab-alumni",
    "title": "NETLAB",
    "section": "Lab Alumni",
    "text": "Lab Alumni\nComing soon…"
  },
  {
    "objectID": "pages/cv.html",
    "href": "pages/cv.html",
    "title": "CVJohn McLevey",
    "section": "",
    "text": "DOWNLOAD PDF VERSION\n\n\n\nYou can download a PDF version of my CV here."
  },
  {
    "objectID": "pages/cv.html#books",
    "href": "pages/cv.html#books",
    "title": "CVJohn McLevey",
    "section": "Books",
    "text": "Books\n\n\n\n2023\n\n\n\n\nJohn McLevey, John Scott, and Peter J. Carrington (Eds). The Sage Handbook of Social Network Analysis, Volume 2. London, UK: Sage.\n\n\n\n\n\n2022\n\n\n\n\nJohn McLevey. Doing Computational Social Science. London, UK: Sage.\n\n\n\n\n\n2022\n\n\n\n\nHarry Collins, Rob Evans, Martin Innes, Will Mason-Wilkes, Eric Kennedy, and John McLevey. The Face to Face Principle and the Internet: Science, Trust, Truth and Democracy. Cardiff, UK: Cardfiff University Press.\n\n\n\n\n\n2020\n\n\n\n\nMark Stoddart, Alice Mattoni, and John McLevey. Industrial Development and Eco-Tourisms: Is Co-existence Possible Between Oil Exploration and Nature Conservation?. London, UK: Palgrave MacMillan."
  },
  {
    "objectID": "pages/cv.html#journal-articles",
    "href": "pages/cv.html#journal-articles",
    "title": "CVJohn McLevey",
    "section": "Journal Articles",
    "text": "Journal Articles\nItalicized authors were students at time of publication.\n\n\n\n2023\n\n\n\n\nDavid Tindall, Nina Kolleck, and John McLevey. “Social Networks and Anthropogenic Climate Change (Special Issue Introduction).” Social Networks. \n\n\n\n\n\n2022\n\n\n\n\nAlexander Graham, John McLevey, Tyler Crick, and Pierson Browne. “Structural Diversity is a Poor Proxy for Information Diversity: Evidence from 25 Scientific Fields.” Social Networks. 70: 55-63. \n\n\n\n\n\n2022\n\n\n\n\nJohn McLevey, Tyler Crick, Pierson Browne, and Darrin Durant. “A New Method for Computational Cultural Cartography: From Neural Word Embeddings to Transformers and Bayesian Mixture Models.” Canadian Review of Sociology / Revue canadienne de sociologie. 59(2): 228-250. \n\n\n\n\n\n2022\n\n\n\n\nDavid Tindall, John McLevey, Yasmin Koop-Monteiro, and Alexander Graham. “Big Data, Computational Social Science, and Other Recent Innovations in Social Network Analysis.” Canadian Review of Sociology / Revue canadienne de sociologie. 59(2): 271-288. \n\n\n\n\n\n2022\n\n\n\n\nAllyson Stokes, Janice Aurini, Jessica Rizk, Rob Gorbet, and John McLevey. “Using Robotics to Support the Acquisition of 21st Century Competencies: Promising (and Practical) Directions.” Canadian Journal of Education / Revue Canadienne De l’éducation. 45(4): 1141–1170. \n\n\n\n\n\n2022\n\n\n\n\nJessica Rizk, Janice Aurini, Allyson Stokes, Rob Gorbet, and John McLevey. “Leading through the COVID-19 Pandemic: Research with Canadian Education Leaders.” Canadian Journal of Educational Administration and Policy. \n\n\n\n\n\n2021\n\n\n\n\nIgor Grossman, Oliver Twardus, Michael E. W. Varnum, Eranda Jayawickreme, and John McLevey. “Expert Predictions of Societal Change: Insights from the World after COVID Project.” American Psychologist. 77(2): 276–290. \n\n\n\n\n\n2021\n\n\n\n\nKathryn S. Plaisance, Janet Michaud, and John McLevey. “Pathways of Influence: Understanding the Impacts of Philosophy of Science in Scientific Domains.” Synthese. 199: 4865–4896. \n\n\n\n\n\n2020\n\n\n\n\nMark Stoddart, John McLevey, Vanessa Schweizer, and Catherine Wong. “Climate Change and Energy Futures: Theoretical Frameworks, Epistemological Issues, and Methodological Perspectives.” Society & Natural Resources. 33(11): 1331-1338. \n\n\n\n\n\n2019\n\n\n\n\nKathryn S. Plaisance, Alexander V. Graham, John McLevey, and Janet Michaud. “Show Me the Numbers: A Quantitative Portrait of the Attitudes, Experiences, and Values of Philosophers of Science.” Synthese. 198: 4603-4633. \n\n\n\n\n\n2018\n\n\n\n\nOwen Gallupe, John McLevey, and Sarah Brown. “Selection or Influence? A Meta-Analysis of the Association between Peer and Personal Offending.” Journal of Quantitative Criminology. 35: 313-335. \n\n\n\n\n\n2018\n\n\n\n\nJohn McLevey, Alexander Graham, Reid McIlroy-Young, Pierson Browne, and Kathryn S. Plaisance. “Interdisciplinarity and Insularity in the Diffusion of Knowledge: An Analysis of Disciplinary Boundaries Between Philosophy of Science and the Sciences.” Scientometrics. 117(1): 331-349. \n\n\n\n\n\n2017\n\n\n\n\nJohn McLevey and Reid McIlroy-Young. “Introducing metaknowledge: Software for Computational Research in Information Science, Network Analysis, and Science of Science.” The Journal of Informetrics. 11: 176-197. \n\n\n\n\n\n2016\n\n\n\n\nAllyson Stokes and John McLevey. “From Porter to Bourdieu: The Evolving Specialty Structure of English Canadian Sociology, 1966- 2014.” Canadian Review of Sociology / Revue canadienne de sociologie. 53(2): 176-202. \n\n\n\n\n\n2015\n\n\n\n\nJohn McLevey. “Understanding Policy Research in Liminal Spaces: Think Tank Responses to Diverging Principles of Legitimacy.” Social Studies of Science. 45(2): 270-293. \n\n\n\n\n\n2014\n\n\n\n\nJohn McLevey. “Think Tanks, Funding, and the Politics of Policy Knowledge in Canada.” Canadian Review of Sociology / Revue canadienne de sociologie. 51(1): 54-75."
  },
  {
    "objectID": "pages/cv.html#chapters-in-edited-volumes",
    "href": "pages/cv.html#chapters-in-edited-volumes",
    "title": "CVJohn McLevey",
    "section": "Chapters in Edited Volumes",
    "text": "Chapters in Edited Volumes\nItalicized authors were students at time of publication.\n\n\n\n2023\n\n\n\n\nPierson Browne, Tyler Crick, and John McLevey. “Inferential Network Clustering with Hierarchical Bayesian Stochastic Blockmodels.” In John McLevey, John Scott, and Peter J. Carrington (Eds) The Sage Handbook of Social Network Analysis, Volume 2. London, UK: Sage.  Open Access Link\n\n\n\n\n\n2023\n\n\n\n\nPierson Browne, Adam Howe, Yasmin Koop-Monteiro, Yixi Yang, and John McLevey. “Scientific Software for Network Analysis.” In John McLevey, John Scott, and Peter J. Carrington (Eds) The Sage Handbook of Social Network Analysis, Volume 2. London, UK: Sage. \n\n\n\n\n\n2023\n\n\n\n\nJohn Scott, John McLevey, and Peter J. Carrington. “Introduction.” In John McLevey, John Scott, and Peter J. Carrington (Eds) The Sage Handbook of Social Network Analysis, Volume 2. London, UK: Sage.  Open Access Link\n\n\n\n\n\n2023\n\n\n\n\nJohn McLevey, Allyson Stokes, and Amelia Howard. “L’influence inégale de Bourdieu sur la sociologie canadienne anglophone (French translation of ‘Pierre Bourdieu’s Uneven Influence on Anglophone Canadian Sociology’).” In Amín Pérez and Franck Poupeau (Eds) Bourdieu dans les Amériques. Genèses et usage d’une internationale scientifique. Aubervilliers, France: l’IHEAL. \n\n\n\n\n\n2021\n\n\n\n\nJohn McLevey and Tyler Crick. “Machine Learning and Neural Network Language Modelling for Sentiment Analysis.” In Luke Sloan and Anabel Quan-Haase (Eds) The Sage Handbook of Social Media Research. London, UK: Sage. \n\n\n\n\n\n2021\n\n\n\n\nJohn McLevey, Pierson Browne, and Tyler Crick. “Reproducibility, Transparency, and Principled Data Processing.” In Uwe Engel and Anabel Quan-Haase (Eds) Handbook of Computational Social Science. New York, US: Routledge. \n\n\n\n\n\n2021\n\n\n\n\nDavid Tindall, Mark Stoddart, John McLevey, Lorien Jasny, Dana R. Fisher, Jennifer Earl, and Mario Diani. “The Challenges and Opportunities of Ego-Network Analysis of Social Movements and Collective Action.” In Mario Small, Brea Perry, Bernice Pescosolido, and Edward Smith (Eds) Personal Networks: Classic Readings and New Directions in Ego-centric Analysis. Cambridge: Cambridge University Press. \n\n\n\n\n\n2018\n\n\n\n\nJohn McLevey, Allyson Stokes, and Amelia Howard. “Pierre Bourdieu’s Uneven Influence on Anglophone Canadian Sociology.” In Thomas Medvetz and Jeff Sallaz (Eds) The Oxford Handbook of Pierre Bourdieu. Oxford: Oxford University Press. \n\n\n\n\n\n2018\n\n\n\n\nJohn McLevey and Ryan Deschamps. “The Sociology of Public Policy Formation and Implementation.” In William Outhwaite and Stephen Turner (Eds) The SAGE Handbook of Political Sociology. London, UK: Sage."
  },
  {
    "objectID": "pages/cv.html#edited-special-issues",
    "href": "pages/cv.html#edited-special-issues",
    "title": "CVJohn McLevey",
    "section": "Edited Special Issues",
    "text": "Edited Special Issues\n\n\n\n2023\n\n\n\n\nDavid Tindall, Nina Kolleck, and John McLevey (Guest Journal Editors) “Social Networks and Anthropogenic Climate Change.” Special issue of Social Networks. \n\n\n\n\n\n2020\n\n\n\n\nMark Stoddart, John McLevey, Vanessa Schweizer, and Catherine Wong (Guest Journal Editors) “Climate Change and Energy Futures.” Special issue of Society & Natural Resources."
  },
  {
    "objectID": "pages/cv.html#article-manuscripts-in-progress",
    "href": "pages/cv.html#article-manuscripts-in-progress",
    "title": "CVJohn McLevey",
    "section": "Article Manuscripts in Progress",
    "text": "Article Manuscripts in Progress\n\n\n\nIn Progress\n\n\n\n\nOwen Gallupe, Noam Gidron, John McLevey, and Annika Hillebrandt. “Unpacking the crime-generating effect of exclusionary political rhetoric.” In Progress. \n\n\n\n\n\nIn Progress\n\n\n\n\nJohn McLevey, Tyler Crick, Darrin Durant, and Karmvir Padda. “How Political Beliefs Form and Evolve in Interaction Networks: A Generative Stance Model.” In Progress. \n\n\n\n\n\nIn Progress\n\n\n\n\nJohn McLevey. “A Generative Method for Constructing and Analyzing Belief Networks from Survey and Text Data.” In Progress."
  },
  {
    "objectID": "pages/cv.html#book-manuscripts-in-progress",
    "href": "pages/cv.html#book-manuscripts-in-progress",
    "title": "CVJohn McLevey",
    "section": "Book Manuscripts in Progress",
    "text": "Book Manuscripts in Progress\n\n\n\nIn Progress\n\n\n\n\nJohn McLevey. “Reproducible Research and Generative Modelling for Social and Cognitive Scientists.” In Progress."
  },
  {
    "objectID": "pages/cv.html#research-policy-reports",
    "href": "pages/cv.html#research-policy-reports",
    "title": "CVJohn McLevey",
    "section": "Research & Policy Reports",
    "text": "Research & Policy Reports\n\n\n\n2020\n\n\n\n\nJohn McLevey, Pierson Browne, Tyler Crick, and Jillian Anderson. “Applied Computer Vision for Disinformation Research: An Analysis of Twitter’s Elections Integrity Data on Russian and Chinese Information Operations.” Prepared for the Crime & Security Research Institute, Cardiff University \n\n\n\n\n\n2020\n\n\n\n\nJohn McLevey, Pierson Browne, and Tyler Crick. “Online Deception & Dynamic Narrative Networks.” Prepared for the Crime & Security Research Institute, Cardiff University \n\n\n\n\n\n2020\n\n\n\n\nRob Gorbet, Janice Aurini, Jessica Risk, Allyson Stokes, John McLevey, and Nicole Figueiredo. “The COVID-19 Pandemic and Canadian Schooling.” Prepared for Education Onward Council in consultation with Fair-Chance Learning \n\n\n\n\n\n2019\n\n\n\n\nJanice Aurini, Rob Gorbet, John McLevey, Jessica Rizk, and Allyson Stokes (Alphabetical). “White Paper on Ed-Tech Connect: An Intersectoral Workshop on Education, Technology, and 21st Century Labour.” Funded by SSHRC Connections Grant \n\n\n\n\n\n2017\n\n\n\n\nJanice Aurini, John McLevey, Allyson Stokes, and Rob Gorbet. “Classroom Robotics and Acquisition of 21st Century Competencies: An Action Research Study of Nine Ontario School Boards.” Prepared for the Council of Ontario Directors of Education (CODE) and the Ministry of Education, Ontario"
  },
  {
    "objectID": "pages/cv.html#reviews-and-other-short-non-refereed-publications",
    "href": "pages/cv.html#reviews-and-other-short-non-refereed-publications",
    "title": "CVJohn McLevey",
    "section": "Reviews and Other Short Non-Refereed Publications",
    "text": "Reviews and Other Short Non-Refereed Publications\n\n\n\n2021\n\n\n\n\nJohn McLevey. “Probabilistic Topic Models.” In Janice Aurini, Melanie Heath, and Stephanie Howells. 2021. The How To of Qualitative Research. London, UK: Sage \n\n\n\n\n\n2019\n\n\n\n\nJohn McLevey. “Epistemic and Evidential Cultures.” In Paul Atkinson, Sara Delamont, Richard Williams, and Alex Cernat (Eds). Sage Research Methods Foundations Online \n\n\n\n\n\n2019\n\n\n\n\nJohn McLevey. “Review of David Johnson (2017) A Fractured Profession: Commercialization and Conflict in Academic Science.” Baltimore: Johns Hopkins University Press. Review published in American Journal of Sociology. 125(1)"
  },
  {
    "objectID": "pages/cv.html#other-contributions-to-scientific-software",
    "href": "pages/cv.html#other-contributions-to-scientific-software",
    "title": "CVJohn McLevey",
    "section": "Other Contributions to Scientific Software",
    "text": "Other Contributions to Scientific Software\n\n\n\nrecordlinkage\n\n\n\n\nContributions to Jonathan de Bruin’s Python package recordlinkage: A Python package for linking records across multiple data sources when there is no unique ID available. NetLab contributions were focused on implementing new comparison and fusion algorithms for my research on the structure and evolution of cross-sectoral collaboration networks in science and technology. Most NetLab contributions to recordlinkage were implemented by Joel Becker (RA) and occasionally Jillian Anderson (RA), and then submitted to Jonathan de Bruin as pull requests."
  },
  {
    "objectID": "pages/cv.html#assigned-courses",
    "href": "pages/cv.html#assigned-courses",
    "title": "CVJohn McLevey",
    "section": "Assigned Courses",
    "text": "Assigned Courses\n\n\n\nWinter 2024\n\n\n\n\nFCIT 607 – Data, Methods, and Models for Future Cities.  Seminar, (N) Students \n\n\n\n\n\nWinter 2024\n\n\n\n\nINTEG 440 – Computational Social Science.  Seminar, (N) Students \n\n\n\n\n\nFall 2023\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, (47) Students \n\n\n\n\n\nFall 2022\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 91 Students \n\n\n\n\n\nWinter 2022\n\n\n\n\nINTEG 240 – Bullshit, Bias, and Bad Arguments.  Lecture, 77 Students \n\n\n\n\n\nFall 2021\n\n\n\n\nINTEG 440 – Computational Social Science.  Seminar, 21 Students \n\n\n\n\n\nFall 2021\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 73 Students \n\n\n\n\n\nFall 2021\n\n\n\n\nINTEG 640 / SOC 712 – Computational Social Science.  Seminar, 20 Students \n\n\n\n\n\nSpring 2021\n\n\n\n\nINTEG 640 / SOC 712 – Computational Social Science.  Seminar, 14 Students \n\n\n\n\n\nWinter 2021\n\n\n\n\nINTEG 440 / SOC 440 – Computational Social Science.  Seminar, 25 Students \n\n\n\n\n\nWinter 2021\n\n\n\n\nINTEG 240 / SOC 230 – Bullshit, Bias, and Bad Arguments.  Lecture, 39 Students \n\n\n\n\n\nFall 2020\n\n\n\n\nINTEG 340 – Research Methods and Design.  Lecture, 20 Students \n\n\n\n\n\nFall 2020\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 87 Students \n\n\n\n\n\nWinter 2020\n\n\n\n\nINTEG 420 (B) – Senior Honours Project (B).  Project, 20 Students \n\n\n\n\n\nWinter 2020\n\n\n\n\nINTEG 440 / 640 – Computational Social Science.  Seminar, 19 Students \n\n\n\n\n\nFall 2019\n\n\n\n\nINTEG 340 – Research Methods and Design.  Seminar, 20 Students \n\n\n\n\n\nFall 2019\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 89 Students \n\n\n\n\n\nWinter 2019\n\n\n\n\nINTEG 420 (B) – Senior Honours Project.  Project, 20 Students \n\n\n\n\n\nWinter 2019\n\n\n\n\nINTEG 440 / 640 – Computational Social Science.  Seminar, 21 Students \n\n\n\n\n\nFall 2018\n\n\n\n\nINTEG 420 (A) – Senior Honours Project (A).  Project, 27 Students \n\n\n\n\n\nFall 2018\n\n\n\n\nINTEG 340 – Research Methods and Design.  Lecture, 18 Students \n\n\n\n\n\nFall 2018\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 53 Students \n\n\n\n\n\nWinter 2018\n\n\n\n\nINTEG 420 (B) – Senior Honours Project (B).  Project, 27 Students \n\n\n\n\n\nWinter 2018\n\n\n\n\nINTEG 340 – Research Methods and Design.  Seminar, 19 Students \n\n\n\n\n\nWinter 2018\n\n\n\n\nINTEG 475 – Big Data and Society.  Seminar, 30 Students \n\n\n\n\n\nFall 2017\n\n\n\n\nINTEG 420 (A) – Senior Honours Project (A).  Project, 28 Students \n\n\n\n\n\nWinter 2017\n\n\n\n\nINTEG 120 – The Art and Science of Learning.  Lecture, 47 Students \n\n\n\n\n\nFall 2016\n\n\n\n\nINTEG 375 – Research Methods and Design.  Seminar, 17 Students \n\n\n\n\n\nFall 2016\n\n\n\n\nINTEG 120 – Disciplines and Integrative Practices.  Lecture, 49 Students \n\n\n\n\n\nWinter 2016\n\n\n\n\nINTEG 375 – Data Visualization.  Seminar, 12 Students \n\n\n\n\n\nWinter 2016\n\n\n\n\nINTEG 275 – Research Methods and Design.  Lecture, 16 Students \n\n\n\n\n\nFall 2015\n\n\n\n\nINTEG 475 – Open Science.  Seminar, 11 Students \n\n\n\n\n\nFall 2015\n\n\n\n\nINTEG 120 – Disciplines and Integrative Practices.  Lecture, 43 Students \n\n\n\n\n\nWinter 2015\n\n\n\n\nINTEG 275 – Creativity and Innovation.  Lecture, 26 Students \n\n\n\n\n\nWinter 2015\n\n\n\n\nINTEG 375 / SOC 312 – Science and Technology in Society.  Seminar, 20 Students \n\n\n\n\n\nFall 2014\n\n\n\n\nINTEG 120 – Disciplines and Integrative Practices.  Lecture, 46 Students \n\n\n\n\n\nWinter 2014\n\n\n\n\nINTEG 375 / SOC 312 – Science and Technology in Society.  Seminar, 15 Students \n\n\n\n\n\nFall 2013\n\n\n\n\nINTEG 120 – Disciplines and Integrative Practices.  Lecture, 55 Students"
  },
  {
    "objectID": "pages/cv.html#reading-courses-not-assigned",
    "href": "pages/cv.html#reading-courses-not-assigned",
    "title": "CVJohn McLevey",
    "section": "Reading Courses (Not Assigned)",
    "text": "Reading Courses (Not Assigned)\n\n\n\nSpring 2022\n\n\n\n\nPolitical Cognition and Polarization  Graduate\n\n\n\n\n\nWinter 2020\n\n\n\n\nNetwork Science  Graduate\n\n\n\n\n\nFall 2020\n\n\n\n\nPolitical Disinformation and Influence Operations  Undergraduate\n\n\n\n\n\nSpring 2018\n\n\n\n\nReproducibility and Replication in the Sciences  Undergraduate\n\n\n\n\n\nSpring 2018\n\n\n\n\nSocial Network Analysis with Python  Graduate\n\n\n\n\n\nWinter 2016\n\n\n\n\nExploratory Data Analysis and Visualization  Graduate\n\n\n\n\n\nWinter 2014\n\n\n\n\nOrganizational Theory  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nDemocracy, Science, and the Politics of Expertise  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nSociology of Science  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nSocial Network Analysis with R  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nData Visualization  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nOrganizational Theory  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nSociology of Organizations: Case Studies  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nSociology of Organizations  Undergraduate\n\n\n\n\n\nSpring 2014\n\n\n\n\nApplied Network Science  Undergraduate"
  },
  {
    "objectID": "pages/cv.html#phd-student-supervision-and-dissertation-committee-membership",
    "href": "pages/cv.html#phd-student-supervision-and-dissertation-committee-membership",
    "title": "CVJohn McLevey",
    "section": "PhD Student Supervision and Dissertation Committee Membership",
    "text": "PhD Student Supervision and Dissertation Committee Membership\n\n\n\nIn Progress\n\n\n\n\nKarmvir Padda – ABD (Sociology & Legal Studies)  John McLevey (Supervisor) + Owen Gallupe and Veronica Kitchen Expected defence in 2025\n\n\n\n\n\nIn Progress\n\n\n\n\nTyler Crick – ABD (Sociology & Legal Studies)  John McLevey (Supervisor) + Owen Gallupe and Peter Carrington Expected defence in 2024\n\n\n\n\n\nIn Progress\n\n\n\n\nAlexander (Sasha) Graham – ABD (Sociology & Legal Studies)  John McLevey (Supervisor) + Owen Gallupe and Peter Carrington Expected defence in 2024\n\n\n\n\n\nIn Progress\n\n\n\n\nJessica Gill – ABD (Sociology & Legal Studies)  Rashmee Singh (Supervisor) + Andrea Quinlan and John McLevey Expected defence in 2025\n\n\n\n\n\nIn Progress\n\n\n\n\nGeisha Sanchez – ABD (Geography & Environmental Management)  Sarah Burch (Supervisor) + Olaf Weber and John McLevey Expected defence in 2024\n\n\n\n\n\nIn Progress\n\n\n\n\nMark Shakespear (Sociology, University of British Columbia)  David Tindall (Supervisor) + K Harrison, A Jorgenson, M Stoddart, and J McLevey \n\n\n\n\n\nIn Progress\n\n\n\n\nYasmin Koop-Monteiro – ABD (Sociology, University of British Columbia)  David Tindall (Supervisor) + Mark Stoddart and John McLevey \n\n\n\n\n\nIn Progress\n\n\n\n\nYixi Yang – ABD (Sociology, Memorial University)  Mark Stoddart (Supervisor) + John McLevey and David Tindall \n\n\n\n\n\nIn Progress\n\n\n\n\nKeegan Aaron Fernandes (Electrical and Computer Engineering)  Dan Davison (Supervisor) + David Wang, Rob Gorbet, and John McLevey \n\n\n\n\n\n2024\n\n\n\n\nEmerson LaCroix (Sociology & Legal Studies)  Janice Aurini (Supervisor) + John McLevey and Stephanie Howells Defended 2024-06\n\n\n\n\n\n2023\n\n\n\n\nPierson Browne (Sociology & Legal Studies)  John McLevey (Supervisor) + Owen Gallupe and Peter Carrington Defended 2023-12\n\n\n\n\n\n2023\n\n\n\n\nPeter Duggins (Systems Design Engineering)  Chris Eliasmith (Supervisor) + Bryan Tripp, Kerstin Dautenhahn, and John McLevey \n\n\n\n\n\n2022\n\n\n\n\nAdam Howe (Sociology, University of British Columbia)  David Tindall (Supervisor) + Catherine Corrigall-Brown, Rima Wilkes, and John McLevey Defended 2022-03\n\n\n\n\n\n2022\n\n\n\n\nFrançois Lapachelle (Sociology, University of British Columbia)  Beth Hirsh (Supervisor) + David Tindall and John McLevey Defended 2022-02\n\n\n\n\n\n2022\n\n\n\n\nJulie Cooke (School of Environment, Enterprise, and Development)  Jennifer Lynes (Supervisor) + Steve Quilley and John McLevey Defended 2022-08\n\n\n\n\n\n2020\n\n\n\n\nRod Missaghian (Sociology & Legal Studies)  Janice Aurini (Supervisor) + Linda Quirke and John McLevey Defended 2020-10\n\n\n\n\n\n2019\n\n\n\n\nBrittany Etmanski (Sociology and Legal Studies)  Janice Aurini (Supervisor) + Owen Gallupe and John McLevey Defended 2019-09\n\n\n\n\n\n2019\n\n\n\n\nMoutasem Zakkar (Applied Health Sciences)  Craig Janes, Samantha Meyer (Supervisor) + Plinio Morita, Daniel Lizotte, John McLevey Defended 2019-11\n\n\n\n\n\n2018\n\n\n\n\nNoorin Manji (Sociology & Legal Studies)  Lorne Dawson (Supervisor) + Janice Aurini and John McLevey Defended 2018-08\n\n\n\n\n\n2016\n\n\n\n\nMichael Clarke (Sociology & Legal Studies)  Kieran Bonner (Supervisor) + David Goodwin and John McLevey Defended 2016-06"
  },
  {
    "objectID": "pages/cv.html#master-of-arts-ma-and-master-of-science-msc-student-theses",
    "href": "pages/cv.html#master-of-arts-ma-and-master-of-science-msc-student-theses",
    "title": "CVJohn McLevey",
    "section": "Master of Arts (MA) and Master of Science (MSc) Student Theses",
    "text": "Master of Arts (MA) and Master of Science (MSc) Student Theses\n\n\n\n2024\n\n\n\n\nNermeen Zia Islam (Balsillie School of International Affairs)  Advisory Role: Supervisor\n\n\n\n\n\n2022\n\n\n\n\nSvetlana Kopan (Sociology and Legal Studies)  Advisory Role: Committee Member\n\n\n\n\n\n2018\n\n\n\n\nNicholas Brendan, MD (School of Public Health Sciences)  Advisory Role: Reader\n\n\n\n\n\n2018\n\n\n\n\nSarah Tang (School of Environment, Enterprise, and Development)  Advisory Role: Committee Member\n\n\n\n\n\n2018\n\n\n\n\nEmerson LaCroix (Sociology and Legal Studies)  Advisory Role: Committee Member\n\n\n\n\n\n2017\n\n\n\n\nAlexander V. Graham (Sociology and Legal Studies)  Advisory Role: Supervisor\n\n\n\n\n\n2017\n\n\n\n\nJunyi (Jill) Wang (Geography and Environmental Management)  Advisory Role: Reader\n\n\n\n\n\n2017\n\n\n\n\nStuart Anderson (Anthropology)  Advisory Role: Reader\n\n\n\n\n\n2016\n\n\n\n\nChen Chen (Geography and Environmental Management)  Advisory Role: Reader"
  },
  {
    "objectID": "pages/cv.html#supervised-undergraduate-honours-theses-design-projects",
    "href": "pages/cv.html#supervised-undergraduate-honours-theses-design-projects",
    "title": "CVJohn McLevey",
    "section": "Supervised Undergraduate Honours Theses & Design Projects",
    "text": "Supervised Undergraduate Honours Theses & Design Projects\n\n\n\n2019-2020\n\n\n\n\nHarrison Lobb (Knowledge Integration) “Rogan to Spencer: Using Discourse Analysis to Diagnose Pathways of Radicalization From the Mainstream to the Reactionary Right” \n\n\n\n\n\n2019-2020\n\n\n\n\nJessica Clark (Knowledge Integration) “Degree Planning Website for University of Waterloo Students” \n\n\n\n\n\n2019-2020\n\n\n\n\nAidan Power (Knowledge Integration) “An Analysis of Integrating Ethics Into Computer Science Curricula: Implications for the Cheriton School of Computer Science” \n\n\n\n\n\n2018-2019\n\n\n\n\nJessilyn Wolfe (Knowledge Integration) “Escaping the Office: Developing Escape Room Experiences to Improve Corporate Small Group Teambuilding” \n\n\n\n\n\n2018-2019\n\n\n\n\nJason Kurian (Knowledge Integration) “Pocket Blues” \n\n\n\n\n\n2017-2018\n\n\n\n\nRachel Wood (Knowledge Integration) “Designing and developing mkd3, software for learning about academic literature” \n\n\n\n\n\n2016-2017\n\n\n\n\nJillian Anderson (Knowledge Integration) “cydr: An R Package for Cleaning Agricultural Yield Data” \n\n\n\n\n\n2015-2016\n\n\n\n\nJulia Yaroshinsky (Sociology & Legal Studies) “Top Contributors of the Linux Kernel: Similarities and Differences” \n\n\n\n\n\n2014-2015\n\n\n\n\nTiffany Lin (Knowledge Integration) “Framing Controversial Internet Technologies: An Exploratory Network Analysis of TOR” \n\n\n\n\n\n2014-2015\n\n\n\n\nBen Carr (Knowledge Integration) “Paradigms, Learning, and Change in Organizations” \n\n\n\n\n\n2013-2014\n\n\n\n\nChristina Minji Chung (Sociology & Legal Studies) “Organizational Dynamics of Tech Startups in the Kitchener-Waterloo Region” \n\n\n\n\n\n2013-2014\n\n\n\n\nChelsea Mills (Knowledge Integration) “Keep the Ideas Coming: How startups encourage creativity and what other organizations can learn from their example”"
  },
  {
    "objectID": "pages/cv.html#highly-qualified-personnel-hqp-training-paid-research-positions",
    "href": "pages/cv.html#highly-qualified-personnel-hqp-training-paid-research-positions",
    "title": "CVJohn McLevey",
    "section": "Highly Qualified Personnel (HQP) Training(Paid Research Positions)",
    "text": "Highly Qualified Personnel (HQP) Training(Paid Research Positions)\n\n\n\n2023\n\n\n\n\nGraduate HQPKarmvir Padda (1 semester), Tyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters) \n\n\n\n\n\n2022\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters) \n\n\n\n\n\n2021\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters) \n\n\n\n\n\n2020\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters) Undergraduate HQPAlex de Witt (3 semesters), Alexis Foss Hill (3 semesters), Harrison Lobb (1 semester)\n\n\n\n\n\n2019\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters) Undergraduate HQPAlex de Witt (1 semester), Alexis Foss Hill (1 semester), Jason Kurian (1 semester)\n\n\n\n\n\n2018\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters), Yixi Yang (3 semesters), Brittany Etmanski (1 semester), Janet Michaud (1 semester) Undergraduate HQPMumtahin Monzoor (1 semester), Rachel Wood (2 semesters)\n\n\n\n\n\n2017\n\n\n\n\nGraduate HQPTyler Crick (3 semesters), Pierson Browne (3 semesters), Alexander Graham (3 semesters), Yixi Yang (3 semesters), Cathlene Hillier (2 semesters), Janet Michaud (1 semester) Undergraduate HQPMumtahin Monzoor (1 semester), Jillian Anderson (2 semesters), Joel Becker (2 semesters), Steve McColl (2 semesters)\n\n\n\n\n\n2016\n\n\n\n\nGraduate HQPBrittany Etmanski (3 semesters), Pierson Browne (3 semesters), Yixi Yang (3 semesters), Cathlene Hillier (3 semesters), Amelia Howard (2 semesters), Alexander Graham (3 semesters) Undergraduate HQPTahnee Prior (2 semesters), Jillian Anderson (2 semesters), Steve McColl (2 semesters), Joel Becker (2 semesters), Reid McIlroy-Young (3 semesters)\n\n\n\n\n\n2015\n\n\n\n\nGraduate HQPAmelia Howard (1 semester), Tahnee Prior (1 semester) Undergraduate HQPAlexander Graham (3 semesters), Tiffany Lin (1 semester), Reid McIlroy-Young (2 semesters)\n\n\n\n\n\n2014\n\n\n\n\nGraduate HQPJanet Michaud (1 semester), Amelia Howard (3 semesters), Benjamin Nelson (1 semester) Undergraduate HQPTiffany Lin (1 semester), Evaleen Hellinga (2 semesters), Alexander V. Graham (3 semesters), 46 KI students (1 day)\n\n\n\n\n\n2013\n\n\n\n\nGraduate HQPSkaidra Puodziunas (1 semester), Alexander V. Graham (1 semester), Evaleen Hellinga (1 semester)"
  },
  {
    "objectID": "pages/cv.html#other-highly-qualified-personnel-hqp-training-workshops-short-courses",
    "href": "pages/cv.html#other-highly-qualified-personnel-hqp-training-workshops-short-courses",
    "title": "CVJohn McLevey",
    "section": "Other Highly Qualified Personnel (HQP) Training(Workshops, Short Courses)",
    "text": "Other Highly Qualified Personnel (HQP) Training(Workshops, Short Courses)\n\n\n\n2024\n\n\n\n\nInstructor for Introduction to Computational Social Science with Python (5 days), GESIS Fall Seminar in Computational Social Science, Leibniz Institute for the Social Sciences, Cologne Germany. Course materials co-designed with Johannes Gruber (University of Amsterdam), who teaches the R course. \n\n\n\n\n\n2018-Present\n\n\n\n\nCo-Instructor (with Eric Kennedy) for Science Outside the Lab (SOtL) North: a week long intensive course in science policy held annually in Ottawa and Montréal or Vancouver and Victoria. Each course cohort is ~ 14 graduate students from science and engineering disciplines across Canada. \n\n\n\n\n\n2020\n\n\n\n\nResearch Methods Workshop (5 days), GESIS Spring Seminar, Leibniz Institute for the Social Sciences, Cologne Germany. Working with Digital Behavioural Data. Co-designed and instructed with Jillian Anderson. \n\n\n\n\n\n2019\n\n\n\n\nResearch Methods Workshop (1 day), Transylvania Summer School in Research Methods. Using Network Analysis to (Partially) Automate Literature Reviews \n\n\n\n\n\n2019\n\n\n\n\nResearch Methods Workshop (2 days) instructed at University of British Columbia, Department of Sociology. Introduction to Big Data and Automated Text Analysis for Social Scientists \n\n\n\n\n\n2019\n\n\n\n\nResearch Methods Workshop (2 days) instructed at University of Waterloo. (Partially) Automating Literature Reviews for Knowledge Synthesis and Discovery \n\n\n\n\n\n2019\n\n\n\n\nResearch Methods Workshop (2 days) instructed at University of Waterloo. How to Analyze Networks with R \n\n\n\n\n\n2019\n\n\n\n\nResearch Methods Workshop (1 day) instructed at University of Waterloo. Automated Content Analysis for Social Scientists"
  },
  {
    "objectID": "pages/cv.html#peer-reviewing---academic-journals",
    "href": "pages/cv.html#peer-reviewing---academic-journals",
    "title": "CVJohn McLevey",
    "section": "Peer Reviewing - Academic Journals",
    "text": "Peer Reviewing - Academic Journals\n\n\nNature Communications\n\n\nAmerican Journal of Sociology\n\n\nAmerican Sociological Review\n\n\nSocial Networks\n\n\nCanadian Review of Sociology\n\n\nScience and Public Policy\n\n\nScientometrics\n\n\nSociological Theory\n\n\nSocial Studies of Science\n\n\nCanadian Journal of Sociology\n\n\nMethodological Innovations\n\n\nActa Sociologica\n\n\nSociety & Natural Resources"
  },
  {
    "objectID": "pages/cv.html#peer-reviewing---books",
    "href": "pages/cv.html#peer-reviewing---books",
    "title": "CVJohn McLevey",
    "section": "Peer Reviewing - Books",
    "text": "Peer Reviewing - Books\n\n\n1 book manuscript for Sage, UK. Quantitative research methods series.\n\n\n1 book manuscript for Sage, UK. Quantitative research methods series."
  },
  {
    "objectID": "pages/cv.html#peer-reviewing-evaluation---grants",
    "href": "pages/cv.html#peer-reviewing-evaluation---grants",
    "title": "CVJohn McLevey",
    "section": "Peer Reviewing / Evaluation - Grants",
    "text": "Peer Reviewing / Evaluation - Grants\n\n\n\n2022\n\n\n\n\nPeer Reviewer for the 2022 SSHRC Insight Grant Awards Competition (Multi-disciplinary).\n\n\n\n\n\n2019\n\n\n\n\nPeer Reviewer for the 2019 SSHRC Insight Grant Awards Competition (Multi-disciplinary).\n\n\n\n\n\n2018\n\n\n\n\nEvaluator for 2017 SSHRC Insight Development Grants Competition.\n\n\n\n\n\n2017\n\n\n\n\nObserver for the 2018 SSHRC Postdoctoral Awards Competition."
  },
  {
    "objectID": "pages/cv.html#peer-reviewing-evaluation---other",
    "href": "pages/cv.html#peer-reviewing-evaluation---other",
    "title": "CVJohn McLevey",
    "section": "Peer Reviewing / Evaluation - Other",
    "text": "Peer Reviewing / Evaluation - Other\n\n\n\n2020\n\n\n\n\nChapter for Luke Sloan and Anabel Quan-Haase (eds). The SAGE Handbook for Social Media Research Methods, 2nd edition. London: Sage.\n\n\n\n\n\n2020\n\n\n\n\nSage Campus Course. (Online research design classes)\n\n\n\n\n\n2018\n\n\n\n\nSage Campus Course. (Online research design classes)"
  },
  {
    "objectID": "pages/cv.html#conference-sessions-organized",
    "href": "pages/cv.html#conference-sessions-organized",
    "title": "CVJohn McLevey",
    "section": "Conference Sessions Organized",
    "text": "Conference Sessions Organized\n\n\n\n2024\n\n\n\n\nSessions on Network Approaches to Attitudes and Beliefs at the 2024 Sunbelt meetings in Edinburgh, Scotland. Co-organized with Claudia Zucca, Mario Diani, and Lorien Jasny\n\n\n\n\n\n2022\n\n\n\n\nSessions on Social Networks and Climate Change at the 2022 Sunbelt meetings in Melbourne, Australia. Co-organized with David Tindall and Mark Stoddart\n\n\n\n\n\n2020\n\n\n\n\nSessions on Disinformation, Computational Methods, Climate Change, and Social Movements at the 2020 Sunbelt meetings in Paris, France (held remotely)\n\n\n\n\n\n2019\n\n\n\n\nSocial Networks and Sociology panel (with Dr. Rochelle Côté), CSA\n\n\n\n\n\n2019\n\n\n\n\nSessions for Social Networks Research Cluster, CSA\n\n\n\n\n\n2019\n\n\n\n\nSessions for Sociology of Science, Technology, and Knowledge Research Cluster, CSA\n\n\n\n\n\n2017\n\n\n\n\nPractical Advice on Communicating Sociological Research panel, CSA\n\n\n\n\n\n2017\n\n\n\n\nSociology of Science and Knowledge, CSA\n\n\n\n\n\n2016\n\n\n\n\nSociology of Science and Knowledge, CSA\n\n\n\n\n\n2015\n\n\n\n\nPolitical Sociology of Science, CSA\n\n\n\n\n\n2015\n\n\n\n\nSociology of Science, CSA\n\n\n\n\n\n2014\n\n\n\n\nSociology of Technology, CSA\n\n\n\n\n\n2014\n\n\n\n\nSociology of Culture (with Dr. Allyson Stokes), CSA"
  },
  {
    "objectID": "pages/cv.html#university-of-waterloo",
    "href": "pages/cv.html#university-of-waterloo",
    "title": "CVJohn McLevey",
    "section": "University of Waterloo",
    "text": "University of Waterloo\n\n\n\n2022-2023\n\n\n\n\nKnowledge Integration Department Chair Search and Selection Committee\n\n\n\n\n\n2023-Present\n\n\n\n\nAssociate Chair (Undergraduate), Department of Knowledge Integration. (Vanessa Schweizer is interim Associate Chair while I am on parental leave.)\n\n\n\n\n\n2022 (07-12)\n\n\n\n\nAssociate Chair (Undergraduate), Department of Knowledge Integration.\n\n\n\n\n\n2021-2022\n\n\n\n\nMember of the Administrative Committee, Faculty of Environment.\n\n\n\n\n\n2021-2022\n\n\n\n\nChair of the Environment Faculty Council, Faculty of Environment.\n\n\n\n\n\n2019-Present\n\n\n\n\nLiaison between the Faculty of Environment and the Cybersecurity and Privacy Institute, University of Waterloo.\n\n\n\n\n\n2020-2022\n\n\n\n\nFaculty Association University of Waterloo (FAUW), Member of Council of Representatives, Department of Knowledge Integration.\n\n\n\n\n\n2019\n\n\n\n\nCommittee member, Tenure and Promotions Committee, Department of Knowledge Integration.\n\n\n\n\n\n2018\n\n\n\n\nSearch and selection committee member for two Canada Research Chairs in Environment and Sustainability, Faculty of Environment.\n\n\n\n\n\n2018\n\n\n\n\nPanelist on the SSHRC Insight Development Grants Panel, organized by Ruth Knechtel from the University of Waterloo Office of Research.\n\n\n\n\n\n2017-2018\n\n\n\n\nCommittee member for university-level ‘Research excellence and crossing disciplinary lines’ committee, which developed a white paper to inform the development of the next University of Waterloo strategic plan. Part of my contribution included a network analysis of inter- and intra-faculty collaboration networks on campus.\n\n\n\n\n\n2016\n\n\n\n\nInformal committee designing a Master’s in Data Science at the University of Waterloo, joint initiative of the Faculty of Arts and Faculty of Environment.\n\n\n\n\n\n2016-2019\n\n\n\n\nMember of Communications Outcomes Committee in the Faculty of Environment.\n\n\n\n\n\n2016\n\n\n\n\nInternal External Examiner for PhD Dissertation in the Department of Applied Mathematics, University of Waterloo. John Lang, ‘Mathematical Modelling of Social Factors in Decision Making Processes at the Individual and Population Levels.’ Dissertation Supervisors: Drs. Hans De Sterck and Daniel M. Abrams.\n\n\n\n\n\n2016\n\n\n\n\nInternal External Examiner for PhD Dissertation in the Department of Philosophy, University of Waterloo. Benjamin Nelson, ‘The Depictions of Unwritten Law.’ Committee Members: Brian Orend, Heather Douglas, and Matt Doucet.\n\n\n\n\n\n2016-Present\n\n\n\n\nMember of the University of Waterloo Task Force on Bibliometrics.\n\n\n\n\n\n2016-2017\n\n\n\n\nCommittee Member on the Faculty of Environment Committee on the University of Waterloo HeForShe Campaign to make gender equality a top institutional priority.\n\n\n\n\n\n2016\n\n\n\n\nCo-organized (with Ian Milligan) a Software Carpentry workshop at the University of Waterloo focusing on shell scripting, git, and Python for social scientific and historical research.\n\n\n\n\n\n2016-2021\n\n\n\n\nFaculty Advisor, University of Waterloo iGEM (International Genetically Engineered Machine) Team, Policy & Practices Subgroup, Mathematical Modeling Subgroup (occasionally). 2016 Gold Medal Winners.\n\n\n\n\n\n2014-Present\n\n\n\n\nBoard Member, University of Waterloo Survey Research Centre\n\n\n\n\n\n2014-2015\n\n\n\n\nFaculty of Environment representative on the Arts Faculty Council\n\n\n\n\n\n2014-2017\n\n\n\n\nDepartment of Knowledge Integration representative on the Environment Faculty Council\n\n\n\n\n\n2014-2015\n\n\n\n\nMember of the English Language Competency Committee in the Faculty of Environment\n\n\n\n\n\n2014\n\n\n\n\nFaculty of Environment Teaching and Learning Committee\n\n\n\n\n\n2014\n\n\n\n\nInternal External Examiner for PhD Dissertation in the Department of Psychology, University of Waterloo. Nathaniel Barr, ‘Reasoned connections: Complex creativity and dual-process theories of cognition.’ Dissertation Advisor: Jonathon Fugelsang."
  },
  {
    "objectID": "pages/cv.html#mcmaster-university",
    "href": "pages/cv.html#mcmaster-university",
    "title": "CVJohn McLevey",
    "section": "McMaster University",
    "text": "McMaster University\n\n\n\n2011-2012\n\n\n\n\nSelection Committee member for two tenure-track hires in Sociology\n\n\n\n\n\n2010-2012\n\n\n\n\nChair of Graduate Student Caucus\n\n\n\n\n\n2011-2012\n\n\n\n\nPhD (ABD) Representative on the Graduate Committee\n\n\n\n\n\n2009-2010\n\n\n\n\nPhD Representative on the Graduate Committee\n\n\n\n\n\n2009-2010\n\n\n\n\nCo-Organizer, Culture and Politics workshop\n\n\n\n\n\n2009-2012\n\n\n\n\nCo-Organizer of Jane Synge Memorial Conferences\n\n\n\n\n\n2009-2012\n\n\n\n\nPeer Mentor in Sociology Peer Mentoring Program\n\n\n\n\n\n2008-2009\n\n\n\n\nUnion Steward, Sociology (CUPE 3906)"
  }
]